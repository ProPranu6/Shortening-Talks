{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "HCluQ_4O-R9d",
        "SPN0_muijnXz",
        "-zVxR8v6fu1y"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCluQ_4O-R9d"
      },
      "source": [
        "# **Mount Your Drive Down**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVHKMu704rr1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8005021-ddd0-4fa6-adad-75015e9da8b8"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPN0_muijnXz"
      },
      "source": [
        "# **SHORTENING TALKS**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npIrS3gM490g"
      },
      "source": [
        "**Shortening Talks Algorithm**\n",
        "* **Method 1 (TF-IDF)**: \n",
        "    * *Layer 1 : KeyWord Extraction using log(1+ Frequency) x log(Lines In Total/Lines Containing word) (TFIDF) Scores*\n",
        "    * *Layer 2 : KeyWord Allotment to Sentences using Lev Ratios*\n",
        "*  **Method 2 (Frequency)**:\n",
        "    * *Layer 1 : KeyWord Extraction  using Frequency Scores*\n",
        "    * *Layer 2 : KeyWord Allotment to Sentences using Lev Ratios*  \n",
        "\n",
        "* **Method 3 (BERT Embedding + BERT Classifier)**:\n",
        "  *  *Layer 1 : Sequence the words in a sentence and pass into BERT*\n",
        "  *  *Layer 2 : Top BERT Classifier to produce important/unimportant sentences*\n",
        "\n",
        "*  **Method 4 (BERT Embedding + LexRank)**:\n",
        "  *  *Layer1 : Sentense to BERT Embeddings*\n",
        "     *Layer 2 : LexRank on sentence vecs and pick sentence vecs above set threshold rank* "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uIvLqqcQ442g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "efdf2a3f-538d-48ad-a75d-afd1c2e9fac1"
      },
      "source": [
        "pip install fuzzywuzzy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting fuzzywuzzy\n",
            "  Downloading https://files.pythonhosted.org/packages/43/ff/74f23998ad2f93b945c0309f825be92e04e0348e062026998b5eefef4c33/fuzzywuzzy-0.18.0-py2.py3-none-any.whl\n",
            "Installing collected packages: fuzzywuzzy\n",
            "Successfully installed fuzzywuzzy-0.18.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oeCqLxq232vv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58a99ca7-1dc9-404d-cb12-e643c2328571"
      },
      "source": [
        "pip install wikipedia\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wikipedia\n",
            "  Downloading https://files.pythonhosted.org/packages/67/35/25e68fbc99e672127cc6fbb14b8ec1ba3dfef035bf1e4c90f78f24a80b7d/wikipedia-1.4.0.tar.gz\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from wikipedia) (4.6.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wikipedia) (2.23.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.0.4)\n",
            "Building wheels for collected packages: wikipedia\n",
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia: filename=wikipedia-1.4.0-cp37-none-any.whl size=11686 sha256=022897f7e84bfa769b5e7926bf9d3e8826029bb2965ea7f26de1f769fd8ad65e\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/2a/18/4e471fd96d12114d16fe4a446d00c3b38fb9efcb744bd31f4a\n",
            "Successfully built wikipedia\n",
            "Installing collected packages: wikipedia\n",
            "Successfully installed wikipedia-1.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GRiGLtuA4Lh6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "516ea3e0-c790-42a5-ea8f-3cd25b494342"
      },
      "source": [
        "import wikipedia\n",
        "from fuzzywuzzy import process\n",
        "from fuzzywuzzy import fuzz\n",
        "import copy\n",
        "import numpy as np\n",
        "import json\n",
        "import copy\n",
        "import nltk\n",
        "import numpy as np\n",
        "import string\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from nltk.corpus import stopwords\n",
        "from fuzzywuzzy import process\n",
        "from fuzzywuzzy import fuzz\n",
        "from scipy import stats\n",
        "from math import *\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "\n",
        "T1path = \"/content/drive/My Drive/AppleTranscript1.txt\" #\"/content/drive/My Drive/transcript1\"\n",
        "T2path = \"/content/drive/My Drive/AppleTranscript2.txt\" #\"/content/drive/My Drive/transcript2\"\n",
        "T3path = \"/content/drive/My Drive/AppleTranscript3.txt\" #\"/content/drive/My Drive/transcript3\"\n",
        "T4path = \"/content/drive/My Drive/AppleTranscript4.txt\" #\"/content/drive/My Drive/transcript4\"\n",
        "Tpath = \"/content/drive/My Drive/entireAppleTranscript.txt\" #\"/content/drive/My Drive/entireTranscript.txt\"\n",
        "\n",
        "transcript = \"\" \n",
        "tpt1 = open(T1path,\"r\")\n",
        "T1 = tpt1.read()\n",
        "T1dict = eval(T1)\n",
        "tpt1.close()\n",
        "\n",
        "tpt2 = open(T2path,\"r\")\n",
        "T2 = tpt2.read()\n",
        "T2dict = eval(T2)\n",
        "tpt2.close()\n",
        "\n",
        "tpt3 = open(T3path,\"r\")\n",
        "T3 = tpt3.read()\n",
        "T3dict = eval(T3)\n",
        "#T3dict = json.loads(T3dict.decode(\"utf-8\"))\n",
        "tpt3.close()\n",
        "\n",
        "tpt4 = open(T4path,\"r\")\n",
        "T4 = tpt4.read()\n",
        "T4dict = eval(T4)\n",
        "#T4dict = json.loads(T4dict.decode(\"utf-8\"))\n",
        "tpt4.close()\n",
        "\n",
        "transcript += T1dict[\"response\"][\"transcript\"]\n",
        "transcript += T2dict[\"response\"][\"transcript\"]\n",
        "transcript += T3dict[\"response\"][\"transcript\"]\n",
        "transcript += T4dict[\"response\"][\"transcript\"]\n",
        "\n",
        "tpt = open(Tpath,\"w\")\n",
        "tpt.write(transcript)\n",
        "tpt.close()\n",
        "\n",
        "#tpt  = open(Tpath,\"r\")\n",
        "Tdicts = [T1dict,T2dict,T3dict,T4dict]\n",
        "#print(tpt.read())       \n",
        "        \n",
        "#RUN THIS CELL WITH SPECIFIED PATH TO LOAD ALL THE TEXT FILE AS STRING INTO \"wholeText\" AND TEXT FILE AS LINES INTO \"lineWiseText\"\n",
        "\n",
        "#path = \"/content/drive/My Drive/TedTranscript.txt\" #Path of the file from the drive\n",
        "path = Tpath # \"/content/drive/My Drive/entireTranscript.txt\"\n",
        "\n",
        "chunk(path,endLineAt=[\".\",\"?\"])               \n",
        "\n",
        "fi = open(path,\"r\")\n",
        "wholeText = fi.read()\n",
        "fi.seek(0)\n",
        "totalWords = len((fi.read()).split())\n",
        "fi.seek(0)\n",
        "totalLines = len(fi.readlines())\n",
        "fi.seek(0)\n",
        "lineWiseText = fi.readlines()\n",
        "fi.close()\n",
        "\n",
        "#print(\"Total Lines present in the Source File is : \", totalLines)\n",
        "#print(\"Total Words present in the source File is : \", totalWords)\n",
        "\n",
        "def superImportant(topic) :\n",
        "\n",
        "  content = wikipedia.summary(topic)\n",
        "  pageObj = wikipedia.WikipediaPage(topic)\n",
        "\n",
        "  hyperLinks1 = []\n",
        "  nonhyperLinks = content.split()\n",
        "  for words2 in pageObj.links:\n",
        "    if words2 in content:\n",
        "      hyperLinks1.append(words2)\n",
        "\n",
        "  contentWords = list(set(copy.deepcopy(hyperLinks1) + nonhyperLinks))\n",
        "  \n",
        "  fails = 0\n",
        "\n",
        "  for words in hyperLinks1:\n",
        "    try :\n",
        "      content = wikipedia.summary(words)\n",
        "      pageObj = wikipedia.WikipediaPage(words)\n",
        "      count =0\n",
        "      for words2 in pageObj.links:\n",
        "        if words2 in content:\n",
        "        \n",
        "          contentWords.append(words2)\n",
        "          count+=1\n",
        "        if count >= 1*len(hyperLinks1):\n",
        "          break\n",
        "    except :\n",
        "      fails +=1\n",
        "  \n",
        "  contentWords = set((\" \".join(contentWords)).split())\n",
        "  return contentWords\n",
        "\n",
        "def changePriorities(dic, mapWords):\n",
        "  frequencies = []\n",
        "  for k,v in dic.items():\n",
        "    frequencies.append(v)\n",
        "  X = copy.deepcopy(np.percentile(np.array(frequencies),95))\n",
        "  \n",
        "  misMatches = 0\n",
        "  for words in mapWords:\n",
        "    if words in dic.keys():\n",
        "      dic[words] = X\n",
        "    elif words.lower() in dic.keys():\n",
        "      dic[words.lower()] = X\n",
        "    else:\n",
        "      misMatches +=1\n",
        "\n",
        "  return dic, misMatches\n",
        "#JUST RUN THIS CELL\n",
        "\n",
        "\n",
        "def chunk(sourceFile, wordsPerLine = None, endLineAt = None ):\n",
        "  fi = open(sourceFile,\"r+\")\n",
        "  text = fi.read()\n",
        "  text = text.replace(\"\\n\",\"\")\n",
        "\n",
        "  \n",
        "\n",
        "  if wordsPerLine != None :\n",
        "   text = text.split()\n",
        "   for words in range(1,len(text)+1):\n",
        "      if words%3 == 0:\n",
        "        text[words-1] = text[words-1] + \"\\n\" \n",
        "   fi.seek(0)\n",
        "   fi.write(\" \".join(text))\n",
        "  if endLineAt != None :\n",
        "    \n",
        "    for words in endLineAt :\n",
        "      text = text.split(words)\n",
        "      text = \"\\n\".join(text)\n",
        "      \n",
        "\n",
        "    fi.seek(0)\n",
        "    fi.write(text)\n",
        "  \n",
        "  fi.close()\n",
        "  return\n",
        "\n",
        "def getKey(D,val): \n",
        "    for key, value in D.items(): \n",
        "         if val == value: \n",
        "             return key \n",
        "    return -1\n",
        "\n",
        "def inNLines(multiLineTxt, word, limitOnDataL = totalLines):\n",
        "  count = 0\n",
        "  multiLineTxt = [sen.lower() for sen in multiLineTxt] \n",
        "  options = process.extract(word,multiLineTxt,limit = limitOnDataL)\n",
        "  \n",
        "  for line,score in options:\n",
        "    thresholdMatchScore = (len(word)/len(line))*100\n",
        "    if (score >=thresholdMatchScore and len(word) >= 3) or (word in line.split()) :\n",
        "      count += 1\n",
        "  return count\n",
        "\n",
        "def completeFiltering(singleStringTxt, multiLineTxt, limitOnFreq, limitOnDataW = 10000):\n",
        "  wholeText = singleStringTxt\n",
        "  cleansed = wholeText.split()[:limitOnDataW]\n",
        "  table = str.maketrans(\"\",\"\",string.punctuation)\n",
        "  cleansed = [w.translate(table) for w in cleansed]\n",
        "  patched = \" \".join(cleansed)\n",
        "  cleansed = patched.split()\n",
        "  cleansed = [words for words in cleansed if not words.lower() in stopwords.words()]\n",
        "  \n",
        "  cleansedTxt = \" \".join(cleansed)\n",
        "\n",
        "  wholeText = [cleansedTxt]\n",
        "  lineWiseText = multiLineTxt\n",
        "\n",
        "  # list of text documents\n",
        "  # create the transform\n",
        "  vectorizer1 = CountVectorizer()\n",
        "  vectorizer2 = CountVectorizer()\n",
        "  # tokenize and build vocab\n",
        "  vectorizer1.fit(wholeText)\n",
        "  vectorizer2.fit(lineWiseText)\n",
        "\n",
        "  # summarize\n",
        "  wToInd1 = vectorizer1.vocabulary_\n",
        "  wToInd2 = vectorizer2.vocabulary_\n",
        "  # encode document\n",
        "  vector1 = vectorizer1.transform(wholeText)\n",
        "  vector2 = vectorizer2.transform(lineWiseText)\n",
        "  # summarize encoded vector\n",
        "  v1 = vector1.toarray()\n",
        "  v2 = vector2.toarray()\n",
        "  \n",
        "  \n",
        "  finalCount = np.sum(v1,axis = 0,keepdims = False)\n",
        "  \n",
        "  countDict1 = dict()\n",
        "  \n",
        "  countDict2 = dict()\n",
        "  countDict3 = dict()\n",
        "  priorities2 = dict()\n",
        "\n",
        "  for ind in range(len(finalCount)):\n",
        "    if finalCount[ind] >=limitOnFreq :\n",
        "      countDict1[getKey(wToInd1,ind)] = finalCount[ind]\n",
        "      countDict3[getKey(wToInd1,ind)] = inNLines(multiLineTxt, getKey(wToInd1,ind))\n",
        "  \n",
        "  for lines in range(v2.shape[0]):\n",
        "    countDict = dict()\n",
        "    for ind in range(v2.shape[1]):\n",
        "      if v2[lines][ind] >=limitOnFreq :\n",
        "        countDict[getKey(wToInd2,ind)] = v2[lines][ind]\n",
        "        \n",
        "    \n",
        "    priorities = sorted(countDict,key=countDict.get,reverse=True)\n",
        "    \n",
        "    countDict2[str(lines+1)] = countDict\n",
        "    priorities2[str(lines+1)] = priorities\n",
        "\n",
        "  contentWords = superImportant(\"Apple Inc\")\n",
        "  countDict1, misMatch = changePriorities(countDict1, contentWords)\n",
        "  print(\"These many got mismatched : \", misMatch)\n",
        "  \n",
        "  priorities1 = sorted(countDict1,key=countDict1.get,reverse=True)\n",
        "  priorities3 = sorted(countDict3, key = countDict3.get, reverse = True)\n",
        "  \n",
        "  \n",
        "  \n",
        "  return priorities1, priorities2, priorities3, countDict1, countDict2, countDict3\n",
        "\n",
        "def fuzzyWayCondense(fileSource, priorities1, priorities2, prioritiesMap1, prioritiesMap2, limitOnLines = 3, limitOnDataL = 100, method = \"Frequency\", printLineScores = False):\n",
        "  \n",
        "  if method == \"Frequency\":\n",
        "    priorities = priorities1\n",
        "    prioritiesMap = prioritiesMap1\n",
        "  elif method == \"TF-IDF\":\n",
        "    priorities = priorities1\n",
        "    prioritiesMap = prioritiesMap1\n",
        "    prioritiesMapext = prioritiesMap2\n",
        "    includeTFIDF = np.zeros((limitOnDataL,len(priorities)))\n",
        "\n",
        "  fi = open(fileSource,\"r\")\n",
        "  include = np.zeros((limitOnDataL,len(priorities)))\n",
        "  \n",
        "  wholeLines = fi.readlines()[:limitOnDataL]\n",
        "  maintain = dict()\n",
        "  \n",
        "  for lines in range(1,limitOnDataL +1):\n",
        "    maintain[str(lines)] = []\n",
        "  \n",
        "  fi.close()\n",
        "  \n",
        "  for words in priorities:\n",
        "    options = process.extract(words,wholeLines,limit = limitOnDataL)\n",
        "    for line,score in options:\n",
        "      thresholdMatchScore = (len(words)/len(line))*100\n",
        "      if ( words in line.split() or (score >=thresholdMatchScore and len(words)>=3) ) and method == \"Frequency\":\n",
        "        maintain[str(wholeLines.index(line)+1)].append(words)\n",
        "        include[wholeLines.index(line)][priorities.index(words)] = 1*prioritiesMap[words]\n",
        "      elif ( words in line.split() or (score>=thresholdMatchScore and len(words)>=3) ) and method == \"TF-IDF\":\n",
        "\n",
        "        maintain[str(wholeLines.index(line)+1)].append(words)\n",
        "        #includeTFIDF[wholeLines.index(line)][priorities.index(words)] = prioritiesMapext[str(wholeLines.index(line)+1)][words]*prioritiesMap[words]\n",
        "        include[wholeLines.index(line)][priorities.index(words)] = log(limitOnDataL/prioritiesMapext[words])*log(1+prioritiesMap[words])*1\n",
        "  \n",
        "  \"\"\" if method == \"TF-IDF\":\n",
        "    \n",
        "    includeTFIDF = list(np.sum(includeTFIDF,axis=0))\n",
        "    \n",
        "    for words in priorities:\n",
        "      options = process.extract(words,wholeLines,limit = limitOnDataL)\n",
        "      for line,score in options:\n",
        "      \n",
        "        if (words in line.split()) :\n",
        "          \n",
        "          include[wholeLines.index(line)][priorities.index(words)] = score*includeTFIDF[priorities.index(words)]\"\"\"\n",
        "\n",
        "  for lines in range(1,limitOnDataL +1):\n",
        "     maintain[str(lines)] = set(maintain[str(lines)] )\n",
        "\n",
        "  include = list(np.sum(include,axis =1))\n",
        "  includeTemp = np.array(copy.deepcopy(include))\n",
        "  \n",
        "  if printLineScores == True :\n",
        "    print(\"\\nThe Scores of the Sentences from 1 to\", limitOnDataL,\" are as follows \\n\", include)\n",
        "    print(\"\\nThe Key Words Per Line for all the lines are : \\n\", maintain)\n",
        " \n",
        "  \n",
        "  \n",
        "  condensedLines = []\n",
        "  condensedLinesIndices = []\n",
        "  if limitOnLines != \"NormSTDPick\" :\n",
        "    includeTemp = (np.sort(includeTemp))[::-1]\n",
        "    for i in range(limitOnLines) :\n",
        "      condensedLines.append(wholeLines[include.index(includeTemp[i])])\n",
        "      condensedLinesIndices.append(include.index(includeTemp[i]) + 1)\n",
        "      include[include.index(includeTemp[i])] = -1\n",
        "  else :\n",
        "    includeTemp = np.array([(value >= np.percentile(includeTemp, 50))for value in includeTemp]).astype(int)\n",
        "    includeTemp = np.reshape(np.argwhere(includeTemp), (-1,)) + 1\n",
        "    condensedLines = [wholeLines[i-1] for i in includeTemp]\n",
        "    condensedLinesIndices = includeTemp\n",
        "\n",
        "  condensedText = \" \".join(condensedLines)\n",
        "\n",
        "  return condensedText, condensedLines, condensedLinesIndices\n",
        "\n",
        "\n",
        "\n",
        "# \"completeFiltering\" func takes \"wholeText\", \"lineWiseText\", \"limitOnFreq\" (let this be unchanged), \"limitOnDataW\" (this equals the \"totalWords\" in above cell)\n",
        "# \"completeFiltering\" func returns priorities1,2 and countDict1,2 which are used more for internal purposes so I'm hiding these outputs\n",
        "priorities1, priorities2, priorities3, countDict1, countDict2, countDict3 = completeFiltering(wholeText, lineWiseText,limitOnFreq = 1, limitOnDataW=totalWords)\n",
        "\n",
        "#print(\"\\nTop Prior Words of 10K Words data : \", priorities1)\n",
        "#print(\"\\nTop Prior Words of every Line data : \", priorities2)\n",
        "\n",
        "# \"fuzzyWayCondense\" func takes \"path\", \"priorities1,2\", \"countDict1,2\", \"limitOnLines\" (this is can be anything <= \"totalLines\"), \"limitonDataL\"(this equals the \"totalLines\" in above cell), \"method\" (let it be unchanged), \"printLineScores\"(let it be False setting it to True just prints scores which are of no use to u)\n",
        "# \"fuzzyWayCondense\" func returns \"condensedText\"(optional use to u), \"condensedLines\"(optional use to u just gives list of line strings) ,\"condensedLinesIndices1(u might need this)\"\n",
        "condensedText, condensedLines, condensedLinesIndices1 = fuzzyWayCondense(path,priorities1, priorities3,countDict1, countDict3, limitOnLines=\"NormSTDPick\", limitOnDataL = totalLines, method = \"TF-IDF\", printLineScores=False)\n",
        "print(\"\\nThis is the TF-IDF Way : \\n\")\n",
        "print(\"\\nThe Original Lines which made thorugh the filtering process  are the line numbers : \\n\", condensedLinesIndices1)\n",
        "#print(\"\\nOverall the condensed Text : \\n\", condensedText)\n",
        "\n",
        "# \"fuzzyWayCondense\" func takes \"path\", \"priorities1,2\", \"countDict1,2\", \"limitOnLines\" (this is can be anything <= \"totalLines\"), \"limitonDataL\"(this equals the \"totalLines\" in above cell), \"method\" (let it be unchanged), \"printLineScores\"(let it be False setting it to True just prints scores which are of no use to u)\n",
        "# \"fuzzyWayCondense\" func returns \"condensedText\"(optional use to u), \"condensedLines\"(optional use to u just gives list of line strings) ,\"condensedLinesIndices2(u might need this)\"\n",
        "#condensedText, condensedLines, condensedLinesIndices2 = fuzzyWayCondense(path,priorities1, priorities2,countDict1, countDict2, limitOnLines=\"NormSTDPick\", limitOnDataL = totalLines, method = \"Frequency\", printLineScores=False)\n",
        "#print(\"\\nThis is the Frequency Way : \\n\")\n",
        "#print(\"\\nThe Original Lines which made thorugh the filtering process  are the line numbers : \\n\", condensedLinesIndices2)\n",
        "#print(\"\\nOverall the condensed Text : \\n\", condensedText)\n",
        "\n",
        "# \"finalSet\" gives union of results of both methods\n",
        "#finalSet =  set(condensedLinesIndices1).union(set(condensedLinesIndices2))\n",
        "finalSet =  set(condensedLinesIndices1)\n",
        "print(\"\\nConsider these lines as Important : \", finalSet )\n",
        "print(\"\\nPercentage of Condensation of initial Text is : {:.4f}%\".format(((totalLines - len(finalSet))/totalLines)*100))\n",
        "\n",
        "\n",
        "Jsonpath = \"/content/drive/My Drive/entireAppleJason.txt\" #\"/content/drive/My Drive/entireJasonObj.txt\"\n",
        "\n",
        "correction = 0\n",
        "for transcripts in range(4):\n",
        "    for times in Tdicts[transcripts][\"response\"][\"words\"]:\n",
        "\n",
        "      times[\"start\"] = times[\"start\"] + correction\n",
        "      times[\"end\"] = times[\"end\"] + correction\n",
        "      mark = times[\"end\"]\n",
        "      \n",
        "    correction =   mark\n",
        "\n",
        "jsonObjs = {\"response\" : dict()}\n",
        "jsonObjs[\"response\"][\"words\"] = T1dict[\"response\"][\"words\"] + T2dict[\"response\"][\"words\"] + T3dict[\"response\"][\"words\"] + T4dict[\"response\"][\"words\"]\n",
        "jsonObjs = str(jsonObjs)\n",
        "json = open(Jsonpath,\"w\")\n",
        "json.write(jsonObjs)\n",
        "json.close()\n",
        "\n",
        "\n",
        "pathr = Jsonpath #\"/content/drive/My Drive/tedxtJasonObject\" # PATH TO THE TEXT FILE CONTAINING JASON OBJECT (DICTIONARY OF REQUEST ID, RESPONSE, TIMESTAMPS OF WORDS ....)\n",
        "fi = open(pathr,\"r\")\n",
        "fullDataDict = eval(fi.read())\n",
        "def convert(seconds): \n",
        "    seconds = seconds % (24 * 3600) \n",
        "    hour = seconds // 3600\n",
        "    seconds %= 3600\n",
        "    minutes = seconds // 60\n",
        "    seconds %= 60\n",
        "      \n",
        "    return \"%d:%02d:%02d\" % (hour, minutes, seconds) \n",
        "\n",
        "wholeWords = wholeText.split()\n",
        "wholeLines = lineWiseText\n",
        "prevWordsCount = dict()\n",
        "wds = 0\n",
        "for sen in wholeLines:\n",
        "     prevWordsCount[str(wholeLines.index(sen)+1)] = wds\n",
        "     wds += len(sen.split())\n",
        "\n",
        "\n",
        "timeStamps = dict()\n",
        "\n",
        "for line in finalSet:\n",
        "     \n",
        "     try:\n",
        "      startInd = prevWordsCount[str(line)]\n",
        "      endInd = prevWordsCount[str(line+1)] -1\n",
        "     # USE \"convert\" if u need the output time stamps to be in HH:MM:SS if not remove \"convert\"\n",
        "      timeRange = (fullDataDict[\"response\"][\"words\"][startInd][\"start\"], fullDataDict[\"response\"][\"words\"][endInd][\"end\"])\n",
        "      timeStamps[int(line)] = timeRange\n",
        "      count =1\n",
        "     except :\n",
        "      print(\"\")\n",
        "\n",
        "c = 0\n",
        "Stamps = []\n",
        "for line in sorted(timeStamps):\n",
        "  c+=1\n",
        "  Stamps.append(timeStamps[line])\n",
        "print(Stamps)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/wikipedia/wikipedia.py:389: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
            "\n",
            "The code that caused this warning is on line 389 of the file /usr/local/lib/python3.7/dist-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
            "\n",
            "  lis = BeautifulSoup(html).find_all('li')\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "These many got mismatched :  492\n",
            "\n",
            "This is the TF-IDF Way : \n",
            "\n",
            "\n",
            "The Original Lines which made thorugh the filtering process  are the line numbers : \n",
            " [  5   6   8  10  12  13  14  15  17  21  22  24  25  28  29  31  32  35\n",
            "  36  39  41  42  43  44  45  47  49  53  54  55  60  62  64  67  70  71\n",
            "  73  75  76  79  85  87  88  90  92  93  94  95  97 100 106 108 110 111\n",
            " 112 113 115 116 120 122 123 124 129 130 131 134 135 136 137 139 143 146\n",
            " 148 149 150 151 154 157 159 160 161 162 163 165 168 170 174 176 178 179\n",
            " 181 183 184 185 189 191 192 193 194 195 197 199 201 202 203 204 206 208\n",
            " 210 215 216 217 219 220 221 222 223 224 225 228 229 230 232 235 238 239\n",
            " 240 241 242 243 244 245 246 247 249 251 252 258 260 261 263 267 268 270\n",
            " 272 273]\n",
            "\n",
            "Consider these lines as Important :  {5, 6, 8, 10, 12, 13, 14, 15, 17, 21, 22, 24, 25, 28, 29, 31, 32, 35, 36, 39, 41, 42, 43, 44, 45, 47, 49, 53, 54, 55, 60, 62, 64, 67, 70, 71, 73, 75, 76, 79, 85, 87, 88, 90, 92, 93, 94, 95, 97, 100, 106, 108, 110, 111, 112, 113, 115, 116, 120, 122, 123, 124, 129, 130, 131, 134, 135, 136, 137, 139, 143, 146, 148, 149, 150, 151, 154, 157, 159, 160, 161, 162, 163, 165, 168, 170, 174, 176, 178, 179, 181, 183, 184, 185, 189, 191, 192, 193, 194, 195, 197, 199, 201, 202, 203, 204, 206, 208, 210, 215, 216, 217, 219, 220, 221, 222, 223, 224, 225, 228, 229, 230, 232, 235, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 249, 251, 252, 258, 260, 261, 263, 267, 268, 270, 272, 273}\n",
            "\n",
            "Percentage of Condensation of initial Text is : 49.4810%\n",
            "[(15.3, 51.7), (51.7, 76.0), (79.1, 92.2), (94.0, 102.8), (104.6, 114.4), (114.4, 126.0), (126.0, 134.2), (134.2, 141.9), (144.8, 151.7), (184.8, 199.3), (201.2, 210.5), (224.7, 229.5), (229.5, 240.1), (263.8, 271.1), (271.1, 275.1), (297.6, 311.7), (311.7, 316.0), (340.2, 346.0), (350.1, 373.2), (376.4, 385.8), (414.6, 432.0), (432.0, 440.6), (440.6, 483.5), (483.5, 494.1), (494.1, 501.4), (532.1, 543.6), (546.3, 557.2), (601.3, 608.5), (608.5, 618.8), (618.8, 634.0999999999999), (671.8, 694.0999999999999), (695.0, 729.8), (734.5999999999999, 738.3), (742.3, 747.0), (754.5, 762.0), (762.0, 780.6999999999999), (783.0, 788.1999999999999), (792.0, 797.3), (797.3, 810.0), (814.6999999999999, 825.6999999999999), (847.6999999999999, 852.4), (854.5999999999999, 864.5999999999999), (864.5999999999999, 873.9), (902.5, 913.6999999999999), (917.5, 928.5), (928.5, 935.6999999999999), (935.6999999999999, 951.5), (951.5, 960.0), (962.0999999999999, 975.3), (979.9, 994.0999999999999), (1057.9, 1062.9), (1064.9, 1075.3), (1076.3, 1087.1), (1087.1, 1096.9), (1096.9, 1102.8), (1102.8, 1124.4), (1129.5, 1138.9), (1138.9, 1147.9), (1152.4, 1159.1), (1160.0, 1183.4), (1183.4, 1195.4), (1195.4, 1204.1999999999998), (1209.4, 1228.6), (1228.6, 1234.8), (1234.8, 1246.5), (1266.0, 1270.1999999999998), (1270.1999999999998, 1309.0), (1309.0, 1314.1999999999998), (1314.1999999999998, 1317.0), (1319.9, 1343.5), (1348.8, 1378.6), (1381.1999999999998, 1397.1), (1399.8, 1406.8), (1406.8, 1412.5), (1412.5, 1423.6999999999998), (1423.6999999999998, 1431.6999999999998), (1451.8, 1459.5), (1483.1, 1487.6), (1489.1999999999998, 1523.0), (1523.0, 1562.0), (1562.0, 1571.1999999999998), (1571.1999999999998, 1579.6), (1579.6, 1593.3), (1597.6999999999998, 1607.3), (1649.0, 1667.6999999999998), (1670.8999999999999, 1679.6), (1692.0, 1734.8), (1738.6, 1742.6), (1755.3999999999999, 1763.3), (1763.3, 1772.8999999999999), (1774.0, 1788.3), (1790.5, 1795.8), (1795.8, 1812.3999999999999), (1812.3999999999999, 1817.5), (1873.8999999999999, 1891.1), (1895.5, 1907.0), (1907.0, 1924.1), (1924.1, 1930.5), (1930.5, 1962.3), (1962.3, 1968.8999999999999), (1973.6999999999998, 1978.1999999999998), (1983.6999999999998, 2003.8), (2005.5, 2014.5), (2014.5, 2019.9), (2019.9, 2026.6), (2026.6, 2040.4), (2042.3, 2051.1), (2054.8, 2063.7), (2069.6, 2092.1), (2133.8, 2151.2), (2151.2, 2165.6), (2165.6, 2172.7), (2174.6, 2197.6), (2197.6, 2202.2), (2202.2, 2205.5), (2205.5, 2214.5), (2214.5, 2220.2), (2220.2, 2230.3), (2230.3, 2244.7), (2263.4, 2282.4), (2282.4, 2289.7), (2289.7, 2298.1), (2300.5, 2310.2), (2316.1, 2320.3), (2325.4, 2330.7), (2330.7, 2341.4), (2341.4, 2346.7), (2346.7, 2353.9), (2353.9, 2359.8), (2359.8, 2369.4), (2369.4, 2385.8), (2385.8, 2392.0), (2392.0, 2402.9), (2402.9, 2411.3), (2413.1, 2415.3), (2417.1, 2447.3), (2447.3, 2471.4), (2487.0, 2499.4), (2503.5, 2515.5), (2520.1, 2527.8), (2530.5, 2533.4), (2544.0, 2553.8), (2553.8, 2559.7), (2561.7, 2571.4), (2573.5, 2619.9), (2619.9, 2633.8)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PwdkLCNs0gc"
      },
      "source": [
        "## **BERT Input Encoding And Model Creation And Execution**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fCvppgC3CAZ2"
      },
      "source": [
        "sample_transcript = \"\"\"0:00a vector is something that has both\n",
        "0:02magnitude and direction magnitude and\n",
        "0:08direction so let's think of an example\n",
        "0:10of what wouldn't and what would be a\n",
        "0:13vector so if someone tells you that\n",
        "0:15something is moving at five miles per\n",
        "0:17hour this information by itself is not a\n",
        "0:20vector quantity it's only it's only\n",
        "0:23specifying a magnitude it's not we don't\n",
        "0:26know what direction this thing is moving\n",
        "0:27five miles per hour in so this right\n",
        "0:30over here which is often referred to as\n",
        "0:32a speed this is a speed is not a vector\n",
        "0:36quantity just by itself this is\n",
        "0:38considered to be a scalar quantity if we\n",
        "0:41want it to be a vector we would also\n",
        "0:43have to specify the direction so for\n",
        "0:46example someone might say it's moving\n",
        "0:48five miles per hour east let's say it's\n",
        "0:51moving five miles per hour due east so\n",
        "0:54now this combined five miles per hour\n",
        "0:56are due east this is a vector quantity\n",
        "0:58and now we wouldn't call it speed\n",
        "1:00anymore we would call it velocity so\n",
        "1:04velocity is a vector we're specifying\n",
        "1:07the magnitude five miles per hour and\n",
        "1:09the direction east but how would can we\n",
        "1:13actually visualize this so let's say\n",
        "1:15we're operating in two dimensions and\n",
        "1:18what's neat about linear algebra is\n",
        "1:19obviously a lot of what applies in two\n",
        "1:21dimensions will extend to three and then\n",
        "1:23even four or five six as many dimensions\n",
        "1:25as we want our brains have trouble\n",
        "1:27visualizing beyond three but what's neat\n",
        "1:29is we can mathematically deal with\n",
        "1:31beyond three using linear algebra and\n",
        "1:33we'll see that in future videos but\n",
        "1:35let's just go back to our straight\n",
        "1:37traditional two-dimensional vector right\n",
        "1:40over here so one way we could represent\n",
        "1:42it as an arrow that is five units long\n",
        "1:45we'll assume that each of our units are\n",
        "1:47our unit here is miles per hour and\n",
        "1:49that's pointed to the right where we'll\n",
        "1:52say the right is east so for example I\n",
        "1:55could start an arrow right over here and\n",
        "1:57I could make its length five the length\n",
        "2:00of the arrow specifies the magnitude so\n",
        "2:03one two three four five and then the\n",
        "2:08direction that the arrow is pointed in\n",
        "2:10specifies its direction\n",
        "2:13so this right over here could could\n",
        "2:16represent visually this vector if we say\n",
        "2:19that the horizontal axis is say East or\n",
        "2:21the positive horizontal Direction is\n",
        "2:23moving in the east this would be West\n",
        "2:25that would be north and then that would\n",
        "2:27be south now it's interesting about\n",
        "2:29vectors is that we only care about the\n",
        "2:31magnitude in the direction we don't\n",
        "2:32necessarily not care where we start\n",
        "2:35where we place it when we think about it\n",
        "2:37visually like this so for example this\n",
        "2:39would be the exact same vector would be\n",
        "2:41equivalent vector to this this vector\n",
        "2:44has the same length so it has the same\n",
        "2:46magnitude has a length of five and its\n",
        "2:49direction is also due east so these two\n",
        "2:51vectors are equivalent now one thing\n",
        "2:54that you might say is well that's fair\n",
        "2:55enough but how do we represent it with a\n",
        "2:57little bit more mathematical notation so\n",
        "2:58we don't have to draw it every time and\n",
        "3:00we can start performing operations on it\n",
        "3:01well the typical way one if you want a\n",
        "3:04variable to represent a vector is\n",
        "3:06usually a lowercase letter if you have\n",
        "3:09if you're publishing a book you can bold\n",
        "3:10it but when you're doing it in your\n",
        "3:12notebook you would typically put a\n",
        "3:13little arrow on top of it and you\n",
        "3:16usually there's several ways that you\n",
        "3:17could do it you could lure say hey five\n",
        "3:19miles per hour east but that doesn't\n",
        "3:20feel like you can really operate on that\n",
        "3:22easily the typical ways to specify if\n",
        "3:25you're in two dimensions to specify two\n",
        "3:27numbers that tell you how much is this\n",
        "3:29vector moving in each of these\n",
        "3:32dimensions so for example this one only\n",
        "3:35moves in the horizontal dimension and so\n",
        "3:37we'll put our horizontal dimension first\n",
        "3:39so you might call this vector five comma\n",
        "3:420 it's moving five positive five in the\n",
        "3:46horizontal direction and it's not moving\n",
        "3:48at all in the vertical direction and the\n",
        "3:51notation might change you might also see\n",
        "3:53notation and actually in the linear\n",
        "3:54algebra context it's more typical to\n",
        "3:56write it as a column vector like this\n",
        "3:58five zero this once again the first\n",
        "4:02coordinate represents how much we're\n",
        "4:04moving in the horizontal direction and\n",
        "4:06the second coordinate represents how\n",
        "4:09much are we moving how much are we\n",
        "4:11moving in the vertical direction now\n",
        "4:15this one isn't that interesting you\n",
        "4:16could have other vectors you could have\n",
        "4:18a vector that looks like this it's\n",
        "4:19moving let's say it's moving three in\n",
        "4:21the horizontal direction\n",
        "4:25and positive 4 so 1 2 3 4 in the\n",
        "4:29vertical direction so let me see it\n",
        "4:31might look something like it might look\n",
        "4:34something like this so this could be\n",
        "4:36another vector right over here\n",
        "4:38maybe we call this vector vector a and\n",
        "4:42once again I want to specify that it is\n",
        "4:44a vector and you see here that if you\n",
        "4:46were to break it down in its Mundi in\n",
        "4:49the horizontal direction it is moving it\n",
        "4:52is moving 3 or it's it's it's it's\n",
        "4:55shifting 3 in the horizontal direction\n",
        "4:58and is shifting 4 positive 4 in the\n",
        "5:01vertical direction so and we get that by\n",
        "5:03literally thinking about how much how\n",
        "5:06much we're moving up and how much we're\n",
        "5:07moving to the right when we start at the\n",
        "5:09at the end of the arrow and go to the\n",
        "5:11front of it so this vector might be\n",
        "5:13specified as 3 3 4 3 4 and you could use\n",
        "5:21a Pythagorean theorem to figure out the\n",
        "5:23actual length of this vector and you'll\n",
        "5:25see because this is a 3 4 5 triangle\n",
        "5:26that this actually has a magnitude of 5\n",
        "5:30and as we study more and more linear\n",
        "5:32algebra we can we're going to start\n",
        "5:34extending these to multiple dimensions\n",
        "5:35obviously we can visualize up to three\n",
        "5:38dimensions and four dimensions it\n",
        "5:39becomes more abstract and that's why\n",
        "5:41this type of a notation is useful\n",
        "5:42because it's very hard to draw for 5 or\n",
        "5:4420 dimensional arrow like this\"\"\"\n",
        "\n",
        "sample_transcript_2 = \"\"\"Let's learn about matrices.\n",
        "What do I mean when I say matrices?\n",
        "Well, matrices is just the plural for matrix, which is\n",
        "probably a word you're familiar with more because of\n",
        "Hollywood than because of mathematics.\n",
        "So what is a matrix?\n",
        "Well, it's actually a pretty simple idea.\n",
        "It's just a table of numbers, that's all a matrix is.\n",
        "So let me draw matrix for you.\n",
        "I don't like that toothpaste-colored blue, so\n",
        "let me use another color.\n",
        "So this is an example of a matrix.\n",
        "I'm going to pick some random numbers out: 5, 1,\n",
        "2, 3, 0, minus 5.\n",
        "That is a matrix.\n",
        "And all it is is a table of numbers.\n",
        "And often times if you wanted to have a variable for a\n",
        "matrix, you'd use a capital letter, so you could use a\n",
        "capital A, and sometimes in some books, they make it extra\n",
        "bold, so like a bold A would be a matrix.\n",
        "And just a little bit of notation, so we would call\n",
        "this matrix, just by convention, you would call\n",
        "this a 2-by-3 matrix.\n",
        "And sometimes they actually write it, 2 by 3, below the\n",
        "bold letter they use to represent the matrix.\n",
        "And what is 2 and what is 3?\n",
        "Well, 2 is the number of rows.\n",
        "We have one row, two rows, right?\n",
        "This is a row.\n",
        "This is a row.\n",
        "And we have three columns: one, two, three.\n",
        "So that's why it's called a 2-by-3 matrix.\n",
        "If I said that B-- I'll do it extra bold-- if B is a 5-by-2\n",
        "matrix, that means that B would have--- I'll just type\n",
        "in numbers: 0, minus 5, 10.\n",
        "So it has five rows and it has two columns, so it'll have\n",
        "another column here.\n",
        "So let's see, minus 10, 3, I'm just putting in random numbers\n",
        "here-- 7, 2, pi.\n",
        "That is a 5-by-2 matrix.\n",
        "So I think you now have kind of a convention that all a\n",
        "matrix is is a table of numbers.\n",
        "You can represent it when you're doing\n",
        "it in variable form.\n",
        "You represent it as a boldface capital letter.\n",
        "Sometimes you'd write a 2 by 3 there.\n",
        "And, you can actually reference the\n",
        "terms of the matrix.\n",
        "So in this example, in the top example where we have matrix\n",
        "A, if someone wanted to reference this element of the\n",
        "matrix, so what is that?\n",
        "That is in the second row.\n",
        "It's in row 2 and it's in column 2, right?\n",
        "This is column 1, this is column 2.\n",
        "This is row 1, row 2.\n",
        "So it's in the second row, second column.\n",
        "So sometimes people will write that A, and they'll write 2\n",
        "comma 2 is equal to 0.\n",
        "Or sometimes they'll write it as a lowercase A 2 comma 2 is\n",
        "equal to 0.\n",
        "These are just the same thing.\n",
        "I'm just doing this to expose you to the notation, because a\n",
        "lot of this really is just notation.\n",
        "So what is a 1 comma 3?\n",
        "Well, that means we're in the first row, the third column.\n",
        "First row, one, two, three, it's this value right here.\n",
        "So that equals 2.\n",
        "So this is just all notation of what a matrix is.\n",
        "It's a table of numbers that can be represented this way\n",
        "and we can represent it as different elements that way.\n",
        "So you might be asking, Sal, well, that's nice.\n",
        "A table of numbers with fancy words and fancy notations, but\n",
        "what is it good for?\n",
        "And that's the interesting point.\n",
        "A matrix is just a data representation.\n",
        "It's just a way of writing down data.\n",
        "That's all it is.\n",
        "it's a table of numbers, but it can be used to represent a\n",
        "whole set of phenomenon.\n",
        "And if soon. you're doing this in your Algebra I or your\n",
        "Algebra II class, you're probably using it to represent\n",
        "linear equations.\n",
        "But we'll learn later-- and I'll do a whole set of videos\n",
        "on applying matrices to a whole bunch\n",
        "of different things.\n",
        "But it can represent-- it's very powerful and if you're\n",
        "doing computer graphics on a matrix, the elements can\n",
        "represent pixels on your screen.\n",
        "They can represent points in coordinate space.\n",
        "They can represent-- who knows?\n",
        "There's tons of things that they can represent.\n",
        "But the important thing to realize is that a matrix, it's\n",
        "not a natural phenomenon.\n",
        "It's not like a lot of the mathematical concepts we've\n",
        "been looking at.\n",
        "It's a way to represent a mathematical concept or a way\n",
        "of representing values, but you kind of have to define\n",
        "what it's representing.\n",
        "But let's put that on the back burner a little bit in terms\n",
        "of what it actually represents.\n",
        "Oh, my wife is here.\n",
        "She's looking for our filing cabinet.\n",
        "But anyway, back to what I was doing.\n",
        "So let's put on the back burner what a matrix is\n",
        "actually representing and let's learn the convention.\n",
        "Because I think, at least initially, that tends to be\n",
        "the hardest part.\n",
        "How do you add matrices?\n",
        "How do you multiply matrices?\n",
        "How do you invert matrices?\n",
        "How do you find the determinant of matrices?\n",
        "All of those words might sound unfamiliar unless you've\n",
        "already been confused by them in your algebra class.\n",
        "I'm going to teach you all of those things first, which are\n",
        "all just really human-defined conventions, and then later\n",
        "on, I'll make a whole bunch of videos on the intuition behind\n",
        "them and what they actually represent.\n",
        "So let's get started.\n",
        "So let's say I wanted to add these two matrices.\n",
        "Let's say the first one-- and let me switch colors.\n",
        "I'll do relatively small ones, just so as not to waste space.\n",
        "Say I have the matrix 3, negative 1, 2 0.\n",
        "Let's call that A, capital A.\n",
        "Let's say matrix B, and I'm just making up numbers.\n",
        "Matrix B is equal to minus 7, 2, 3, 5.\n",
        "So my question to you is what is A-- so I'm doing it bold\n",
        "like they do in the textbooks-- plus matrix B?\n",
        "So I'm adding two matrices.\n",
        "And once again, this is just human convention.\n",
        "Someone defined how matrices add.\n",
        "They could've defined it some other way, but they said we're\n",
        "going to make matrices add the way I'm about to show you,\n",
        "because it's useful for a whole set of phenomena.\n",
        "So when you add two matrices, you essentially just add the\n",
        "corresponding elements.\n",
        "So how does that work?\n",
        "Well, you add the element that is in row 1, column 1 to the\n",
        "elements in row 1, column 1, right?\n",
        "So it's 3 plus minus 7.\n",
        "That'll be the 1, 1 element.\n",
        "Then the row 1, column 2 element will be\n",
        "minus 1 plus 2.\n",
        "Let me put parentheses around them so you know these are\n",
        "separate elements.\n",
        "And you can guess how this keeps going.\n",
        "This element will be 2 plus 3 and this last element\n",
        "will be 0 plus 5.\n",
        "So that equals 3 plus minus 7.\n",
        "That is minus 4.\n",
        "Minus 1 plus 2, that's 1.\n",
        "2 plus 3 is 5, and 0 plus 5 is 5.\n",
        "So there we have it.\n",
        "That is how we humans have defined the\n",
        "addition of two matrices.\n",
        "And by this definition, you could imagine that this is\n",
        "going to be the same thing as B plus A, right?\n",
        "And remember, this is something we have to think\n",
        "about because we're not adding numbers anymore.\n",
        "You know 1 plus 2 is the same thing you is 2 plus 1, or you\n",
        "know any two normal numbers, it doesn't matter what order\n",
        "you add them in.\n",
        "But with matrices, it's not completely obvious.\n",
        "But when you define it in this way, it doesn't matter if we\n",
        "do A plus B or B plus A, right?\n",
        "If we did B plus A, this would just say negative 7 plus 3.\n",
        "This would say 2 plus negative 1.\n",
        "But it would come out to the same values.\n",
        "That is matrix addition.\n",
        "And you could imagine, matrix subtraction, it's essentially\n",
        "the same thing.\n",
        "Well, actually let me show you.\n",
        "What would be A minus B?\n",
        "Well, this is capital B with the matrix, so I'm making it\n",
        "extra bold.\n",
        "Well, that's the same thing as A plus minus 1 times B.\n",
        "And what's B?\n",
        "Well, B is minus 7, 2, 3, 5.\n",
        "When you multiply a scalar, when you just multiply a\n",
        "number times a matrix, you just multiply the number times\n",
        "every one of it's elements.\n",
        "So that equals A, matrix A, plus the matrix.\n",
        "We just multiply this negative 1 times every element in here.\n",
        "So 7, minus 2, minus 3, and 5.\n",
        "And then we can do what we did up there.\n",
        "And we know what A is, so this would equal-- A is up here, so\n",
        "3 plus 7 is 10.\n",
        "Negative 1 plus negative 2 is minus 3, 2 plus minus 3 is\n",
        "minus 1, and 0 plus 5 is 5.\n",
        "Now, you didn't even have to go through this\n",
        "exercise right here.\n",
        "You could have literally just subtracted these elements from\n",
        "these elements, and you would have gotten the same value.\n",
        "I did this because I wanted to show you also that multiplying\n",
        "a scalar or just a value or a number times a matrix is just\n",
        "multiplying that number times all of the\n",
        "elements of the matrix.\n",
        "And so by this definition of matrix\n",
        "addition, what do we know?\n",
        "Well, we know that both matrices have to be the same\n",
        "size by this definition, the way we're adding.\n",
        "So, for example, you could add these two matrices.\n",
        "You could add 1, 2, 3, 4, 5, 6, 7, 8, 9 to this matrix, to\n",
        "minus 10, minus 100, minus 1000-- I'm making up numbers--\n",
        "1, 0, 0, 1, 0, 1.\n",
        "You can add these two matrices, right?\n",
        "Because they have the same number of rows and same number\n",
        "of columns.\n",
        "So for example, if you were to add them, the first term up\n",
        "here would be 1 plus minus 10, so it would be minus 9.\n",
        "2 plus minus 100 is minus 98.\n",
        "I think you get the point.\n",
        "You would have exactly nine elements and you would have\n",
        "three rows and three columns.\n",
        "But you could not add these two matrices.\n",
        "You could not add-- Let me do it in a different color just\n",
        "so it's different.\n",
        "You could not add this blue.\n",
        "You could not add this matrix: minus 3, 2 to the matrix 9, 7.\n",
        "And why can you not add them?\n",
        "Well, they don't have corresponding\n",
        "elements to add up.\n",
        "This is a one row by two column, this is 1 by 2, and\n",
        "this is 2 by 1.\n",
        "So they don't have the same dimensions so we can't add or\n",
        "subtract these matrices.\n",
        "And this is a side note, when a matrix has-- when one of its\n",
        "dimensions is one-- so, for example, here you have one row\n",
        "and multiple columns, this is actually called a row vector.\n",
        "A vector is essentially a one-dimensional matrix where\n",
        "one of the dimensions is one.\n",
        "So this is a row vector, and similarly,\n",
        "this is a column vector.\n",
        "That's just a little extra terminology you should know.\n",
        "If you take linear algebra and calculus, your professor might\n",
        "use those terms and it's good to be familiar with it.\n",
        "But anyway, I'm pushing 11 minutes, so I will continue\n",
        "this in the next video.\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JzJSk96sDwss",
        "outputId": "525589f5-9fc6-44eb-d2a4-2bb23d538415"
      },
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "sens = re.sub(\"[0-9]+:[0-9]+\", \"\", sample_transcript_2)\n",
        "sens = sens.split(\"\\n\")\n",
        "\n",
        "importance = []\n",
        "for sen in sens:\n",
        "  response = int(input(\"( \" + sen + \" )\" + \" imp? \"))#random.randint(0,1)#\n",
        "  importance.append(response)\n",
        "\n",
        "cols_dict = {\"transcript\":sens, \"importance\":importance}\n",
        "transcript_csv = pd.DataFrame(cols_dict)\n",
        "transcript_csv.to_csv(\"/content/drive/MyDrive/Short Talks BERT/train data csv/khan_academy_02.csv\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "( Let's learn about matrices. ) imp? 0\n",
            "( What do I mean when I say matrices? ) imp? 0\n",
            "( Well, matrices is just the plural for matrix, which is ) imp? 1\n",
            "( probably a word you're familiar with more because of ) imp? 0\n",
            "( Hollywood than because of mathematics. ) imp? 0\n",
            "( So what is a matrix? ) imp? 1\n",
            "( Well, it's actually a pretty simple idea. ) imp? 0\n",
            "( It's just a table of numbers, that's all a matrix is. ) imp? 1\n",
            "( So let me draw matrix for you. ) imp? 0\n",
            "( I don't like that toothpaste-colored blue, so ) imp? 0\n",
            "( let me use another color. ) imp? 0\n",
            "( So this is an example of a matrix. ) imp? 1\n",
            "( I'm going to pick some random numbers out: 5, 1, ) imp? 1\n",
            "( 2, 3, 0, minus 5. ) imp? 1\n",
            "( That is a matrix. ) imp? 1\n",
            "( And all it is is a table of numbers. ) imp? 1\n",
            "( And often times if you wanted to have a variable for a ) imp? 1\n",
            "( matrix, you'd use a capital letter, so you could use a ) imp? 1\n",
            "( capital A, and sometimes in some books, they make it extra ) imp? 1\n",
            "( bold, so like a bold A would be a matrix. ) imp? 1\n",
            "( And just a little bit of notation, so we would call ) imp? 0\n",
            "( this matrix, just by convention, you would call ) imp? 0\n",
            "( this a 2-by-3 matrix. ) imp? 1\n",
            "( And sometimes they actually write it, 2 by 3, below the ) imp? 0\n",
            "( bold letter they use to represent the matrix. ) imp? 0\n",
            "( And what is 2 and what is 3? ) imp? 1\n",
            "( Well, 2 is the number of rows. ) imp? 1\n",
            "( We have one row, two rows, right? ) imp? 0\n",
            "( This is a row. ) imp? 1\n",
            "( This is a row. ) imp? 1\n",
            "( And we have three columns: one, two, three. ) imp? 1\n",
            "( So that's why it's called a 2-by-3 matrix. ) imp? 1\n",
            "( If I said that B-- I'll do it extra bold-- if B is a 5-by-2 ) imp? 1\n",
            "( matrix, that means that B would have--- I'll just type ) imp? 1\n",
            "( in numbers: 0, minus 5, 10. ) imp? 1\n",
            "( So it has five rows and it has two columns, so it'll have ) imp? 1\n",
            "( another column here. ) imp? 1\n",
            "( So let's see, minus 10, 3, I'm just putting in random numbers ) imp? 0\n",
            "( here-- 7, 2, pi. ) imp? 0\n",
            "( That is a 5-by-2 matrix. ) imp? 1\n",
            "( So I think you now have kind of a convention that all a ) imp? 0\n",
            "( matrix is is a table of numbers. ) imp? 1\n",
            "( You can represent it when you're doing ) imp? 0\n",
            "( it in variable form. ) imp? 0\n",
            "( You represent it as a boldface capital letter. ) imp? 1\n",
            "( Sometimes you'd write a 2 by 3 there. ) imp? 0\n",
            "( And, you can actually reference the ) imp? 0\n",
            "( terms of the matrix. ) imp? 0\n",
            "( So in this example, in the top example where we have matrix ) imp? 0\n",
            "( A, if someone wanted to reference this element of the ) imp? 1\n",
            "( matrix, so what is that? ) imp? 1\n",
            "( That is in the second row. ) imp? 1\n",
            "( It's in row 2 and it's in column 2, right? ) imp? 1\n",
            "( This is column 1, this is column 2. ) imp? 1\n",
            "( This is row 1, row 2. ) imp? 1\n",
            "( So it's in the second row, second column. ) imp? 1\n",
            "( So sometimes people will write that A, and they'll write 2 ) imp? 1\n",
            "( comma 2 is equal to 0. ) imp? 1\n",
            "( Or sometimes they'll write it as a lowercase A 2 comma 2 is ) imp? 0\n",
            "( equal to 0. ) imp? 0\n",
            "( These are just the same thing. ) imp? 0\n",
            "( I'm just doing this to expose you to the notation, because a ) imp? 0\n",
            "( lot of this really is just notation. ) imp? 0\n",
            "( So what is a 1 comma 3? ) imp? 1\n",
            "( Well, that means we're in the first row, the third column. ) imp? 1\n",
            "( First row, one, two, three, it's this value right here. ) imp? 0\n",
            "( So that equals 2. ) imp? 0\n",
            "( So this is just all notation of what a matrix is. ) imp? 1\n",
            "( It's a table of numbers that can be represented this way ) imp? 1\n",
            "( and we can represent it as different elements that way. ) imp? 1\n",
            "( So you might be asking, Sal, well, that's nice. ) imp? 0\n",
            "( A table of numbers with fancy words and fancy notations, but ) imp? 0\n",
            "( what is it good for? ) imp? 1\n",
            "( And that's the interesting point. ) imp? 0\n",
            "( A matrix is just a data representation. ) imp? 1\n",
            "( It's just a way of writing down data. ) imp? 1\n",
            "( That's all it is. ) imp? 0\n",
            "( it's a table of numbers, but it can be used to represent a ) imp? 1\n",
            "( whole set of phenomenon. ) imp? 1\n",
            "( And if soon. you're doing this in your Algebra I or your ) imp? 0\n",
            "( Algebra II class, you're probably using it to represent ) imp? 0\n",
            "( linear equations. ) imp? 1\n",
            "( But we'll learn later-- and I'll do a whole set of videos ) imp? 0\n",
            "( on applying matrices to a whole bunch ) imp? 0\n",
            "( of different things. ) imp? 0\n",
            "( But it can represent-- it's very powerful and if you're ) imp? 1\n",
            "( doing computer graphics on a matrix, the elements can ) imp? 1\n",
            "( represent pixels on your screen. ) imp? 1\n",
            "( They can represent points in coordinate space. ) imp? 1\n",
            "( They can represent-- who knows? ) imp? 0\n",
            "( There's tons of things that they can represent. ) imp? 1\n",
            "( But the important thing to realize is that a matrix, it's ) imp? 1\n",
            "( not a natural phenomenon. ) imp? 1\n",
            "( It's not like a lot of the mathematical concepts we've ) imp? 1\n",
            "( been looking at. ) imp? 1\n",
            "( It's a way to represent a mathematical concept or a way ) imp? 1\n",
            "( of representing values, but you kind of have to define ) imp? 1\n",
            "( what it's representing. ) imp? 1\n",
            "( But let's put that on the back burner a little bit in terms ) imp? 0\n",
            "( of what it actually represents. ) imp? 0\n",
            "( Oh, my wife is here. ) imp? 0\n",
            "( She's looking for our filing cabinet. ) imp? 0\n",
            "( But anyway, back to what I was doing. ) imp? 0\n",
            "( So let's put on the back burner what a matrix is ) imp? 0\n",
            "( actually representing and let's learn the convention. ) imp? 0\n",
            "( Because I think, at least initially, that tends to be ) imp? 0\n",
            "( the hardest part. ) imp? 0\n",
            "( How do you add matrices? ) imp? 1\n",
            "( How do you multiply matrices? ) imp? 1\n",
            "( How do you invert matrices? ) imp? 1\n",
            "( How do you find the determinant of matrices? ) imp? 1\n",
            "( All of those words might sound unfamiliar unless you've ) imp? 1\n",
            "( already been confused by them in your algebra class. ) imp? 1\n",
            "( I'm going to teach you all of those things first, which are ) imp? 0\n",
            "( all just really human-defined conventions, and then later ) imp? 1\n",
            "( on, I'll make a whole bunch of videos on the intuition behind ) imp? 0\n",
            "( them and what they actually represent. ) imp? 0\n",
            "( So let's get started. ) imp? 0\n",
            "( So let's say I wanted to add these two matrices. ) imp? 1\n",
            "( Let's say the first one-- and let me switch colors. ) imp? 1\n",
            "( I'll do relatively small ones, just so as not to waste space. ) imp? 0\n",
            "( Say I have the matrix 3, negative 1, 2 0. ) imp? 1\n",
            "( Let's call that A, capital A. ) imp? 1\n",
            "( Let's say matrix B, and I'm just making up numbers. ) imp? 1\n",
            "( Matrix B is equal to minus 7, 2, 3, 5. ) imp? 1\n",
            "( So my question to you is what is A-- so I'm doing it bold ) imp? 1\n",
            "( like they do in the textbooks-- plus matrix B? ) imp? 0\n",
            "( So I'm adding two matrices. ) imp? 1\n",
            "( And once again, this is just human convention. ) imp? 0\n",
            "( Someone defined how matrices add. ) imp? 0\n",
            "( They could've defined it some other way, but they said we're ) imp? 0\n",
            "( going to make matrices add the way I'm about to show you, ) imp? 0\n",
            "( because it's useful for a whole set of phenomena. ) imp? 1\n",
            "( So when you add two matrices, you essentially just add the ) imp? 1\n",
            "( corresponding elements. ) imp? 1\n",
            "( So how does that work? ) imp? 1\n",
            "( Well, you add the element that is in row 1, column 1 to the ) imp? 1\n",
            "( elements in row 1, column 1, right? ) imp? 1\n",
            "( So it's 3 plus minus 7. ) imp? 1\n",
            "( That'll be the 1, 1 element. ) imp? 1\n",
            "( Then the row 1, column 2 element will be ) imp? 1\n",
            "( minus 1 plus 2. ) imp? 1\n",
            "( Let me put parentheses around them so you know these are ) imp? 0\n",
            "( separate elements. ) imp? 0\n",
            "( And you can guess how this keeps going. ) imp? 1\n",
            "( This element will be 2 plus 3 and this last element ) imp? 1\n",
            "( will be 0 plus 5. ) imp? 1\n",
            "( So that equals 3 plus minus 7. ) imp? 1\n",
            "( That is minus 4. ) imp? 1\n",
            "( Minus 1 plus 2, that's 1. ) imp? 1\n",
            "( 2 plus 3 is 5, and 0 plus 5 is 5. ) imp? 1\n",
            "( So there we have it. ) imp? 0\n",
            "( That is how we humans have defined the ) imp? 0\n",
            "( addition of two matrices. ) imp? 0\n",
            "( And by this definition, you could imagine that this is ) imp? 0\n",
            "( going to be the same thing as B plus A, right? ) imp? 1\n",
            "( And remember, this is something we have to think ) imp? 0\n",
            "( about because we're not adding numbers anymore. ) imp? 1\n",
            "( You know 1 plus 2 is the same thing you is 2 plus 1, or you ) imp? 1\n",
            "( know any two normal numbers, it doesn't matter what order ) imp? 1\n",
            "( you add them in. ) imp? 1\n",
            "( But with matrices, it's not completely obvious. ) imp? 1\n",
            "( But when you define it in this way, it doesn't matter if we ) imp? 1\n",
            "( do A plus B or B plus A, right? ) imp? 1\n",
            "( If we did B plus A, this would just say negative 7 plus 3. ) imp? 1\n",
            "( This would say 2 plus negative 1. ) imp? 1\n",
            "( But it would come out to the same values. ) imp? 1\n",
            "( That is matrix addition. ) imp? 1\n",
            "( And you could imagine, matrix subtraction, it's essentially ) imp? 1\n",
            "( the same thing. ) imp? 1\n",
            "( Well, actually let me show you. ) imp? 0\n",
            "( What would be A minus B? ) imp? 1\n",
            "( Well, this is capital B with the matrix, so I'm making it ) imp? 1\n",
            "( extra bold. ) imp? 0\n",
            "( Well, that's the same thing as A plus minus 1 times B. ) imp? 1\n",
            "( And what's B? ) imp? 1\n",
            "( Well, B is minus 7, 2, 3, 5. ) imp? 1\n",
            "( When you multiply a scalar, when you just multiply a ) imp? 1\n",
            "( number times a matrix, you just multiply the number times ) imp? 1\n",
            "( every one of it's elements. ) imp? 1\n",
            "( So that equals A, matrix A, plus the matrix. ) imp? 1\n",
            "( We just multiply this negative 1 times every element in here. ) imp? 1\n",
            "( So 7, minus 2, minus 3, and 5. ) imp? 1\n",
            "( And then we can do what we did up there. ) imp? 1\n",
            "( And we know what A is, so this would equal-- A is up here, so ) imp? 0\n",
            "( 3 plus 7 is 10. ) imp? 1\n",
            "( Negative 1 plus negative 2 is minus 3, 2 plus minus 3 is ) imp? 1\n",
            "( minus 1, and 0 plus 5 is 5. ) imp? 1\n",
            "( Now, you didn't even have to go through this ) imp? 0\n",
            "( exercise right here. ) imp? 0\n",
            "( You could have literally just subtracted these elements from ) imp? 0\n",
            "( these elements, and you would have gotten the same value. ) imp? 0\n",
            "( I did this because I wanted to show you also that multiplying ) imp? 0\n",
            "( a scalar or just a value or a number times a matrix is just ) imp? 0\n",
            "( multiplying that number times all of the ) imp? 0\n",
            "( elements of the matrix. ) imp? 0\n",
            "( And so by this definition of matrix ) imp? 1\n",
            "( addition, what do we know? ) imp? 1\n",
            "( Well, we know that both matrices have to be the same ) imp? 1\n",
            "( size by this definition, the way we're adding. ) imp? 1\n",
            "( So, for example, you could add these two matrices. ) imp? 1\n",
            "( You could add 1, 2, 3, 4, 5, 6, 7, 8, 9 to this matrix, to ) imp? 1\n",
            "( minus 10, minus 100, minus 1000-- I'm making up numbers-- ) imp? 1\n",
            "( 1, 0, 0, 1, 0, 1. ) imp? 1\n",
            "( You can add these two matrices, right? ) imp? 1\n",
            "( Because they have the same number of rows and same number ) imp? 1\n",
            "( of columns. ) imp? 1\n",
            "( So for example, if you were to add them, the first term up ) imp? 1\n",
            "( here would be 1 plus minus 10, so it would be minus 9. ) imp? 1\n",
            "( 2 plus minus 100 is minus 98. ) imp? 1\n",
            "( I think you get the point. ) imp? 0\n",
            "( You would have exactly nine elements and you would have ) imp? 1\n",
            "( three rows and three columns. ) imp? 1\n",
            "( But you could not add these two matrices. ) imp? 1\n",
            "( You could not add-- Let me do it in a different color just ) imp? 0\n",
            "( so it's different. ) imp? 1\n",
            "( You could not add this blue. ) imp? 0\n",
            "( You could not add this matrix: minus 3, 2 to the matrix 9, 7. ) imp? 1\n",
            "( And why can you not add them? ) imp? 1\n",
            "( Well, they don't have corresponding ) imp? 1\n",
            "( elements to add up. ) imp? 1\n",
            "( This is a one row by two column, this is 1 by 2, and ) imp? 1\n",
            "( this is 2 by 1. ) imp? 1\n",
            "( So they don't have the same dimensions so we can't add or ) imp? 1\n",
            "( subtract these matrices. ) imp? 1\n",
            "( And this is a side note, when a matrix has-- when one of its ) imp? 1\n",
            "( dimensions is one-- so, for example, here you have one row ) imp? 1\n",
            "( and multiple columns, this is actually called a row vector. ) imp? 1\n",
            "( A vector is essentially a one-dimensional matrix where ) imp? 1\n",
            "( one of the dimensions is one. ) imp? 1\n",
            "( So this is a row vector, and similarly, ) imp? 1\n",
            "( this is a column vector. ) imp? 1\n",
            "( That's just a little extra terminology you should know. ) imp? 1\n",
            "( If you take linear algebra and calculus, your professor might ) imp? 0\n",
            "( use those terms and it's good to be familiar with it. ) imp? 0\n",
            "( But anyway, I'm pushing 11 minutes, so I will continue ) imp? 0\n",
            "( this in the next video. ) imp? 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZATdGeklF4x"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "transcript_csv = pd.read_csv(\"/content/drive/MyDrive/Short Talks BERT/train data csv/khan_academy_01.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebrxeSJJaQDY"
      },
      "source": [
        "## **Setting Up BERT**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lULyNsDAVVus",
        "outputId": "266d9d2a-879c-4fd2-ab6a-a83bdf87429e"
      },
      "source": [
        "pip install -q tf-models-official==2.4.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 1.1 MB 9.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 352 kB 44.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 43 kB 1.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 679 kB 41.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 44.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 37.1 MB 49 kB/s \n",
            "\u001b[K     |████████████████████████████████| 99 kB 8.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 636 kB 34.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 211 kB 50.7 MB/s \n",
            "\u001b[?25h  Building wheel for py-cpuinfo (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HfQABTrVVjMR"
      },
      "source": [
        "from official.modeling import tf_utils\n",
        "from official import nlp\n",
        "from official.nlp import bert\n",
        "\n",
        "# Load the required submodules\n",
        "import official.nlp.optimization\n",
        "import official.nlp.bert.bert_models\n",
        "import official.nlp.bert.configs\n",
        "import official.nlp.bert.run_classifier\n",
        "import official.nlp.bert.tokenization\n",
        "import official.nlp.data.classifier_data_lib\n",
        "import official.nlp.modeling.losses\n",
        "import official.nlp.modeling.models\n",
        "import official.nlp.modeling.networks"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ngcjCrSwT0Dm",
        "outputId": "bb995504-f010-4d74-c150-d387907d1748"
      },
      "source": [
        "import tensorflow as tf\n",
        "gs_folder_bert = \"gs://cloud-tpu-checkpoints/bert/v3/uncased_L-12_H-768_A-12\"\n",
        "tf.io.gfile.listdir(gs_folder_bert)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['bert_config.json',\n",
              " 'bert_model.ckpt.data-00000-of-00001',\n",
              " 'bert_model.ckpt.index',\n",
              " 'vocab.txt']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PpKYxO9PUExK",
        "outputId": "b446fa07-3380-4f51-b509-6683e6557e82"
      },
      "source": [
        "transcript_csv_lab = transcript_csv.importance\n",
        "transcript_csv_feat = transcript_csv.transcript \n",
        "transcript_ds = tf.data.Dataset.from_tensor_slices((transcript_csv_feat, transcript_csv_lab))\n",
        "\n",
        "for trans, lab in transcript_ds.batch(2):\n",
        "  print(trans)\n",
        "  break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[b'a vector is something that has both'\n",
            " b'magnitude and direction magnitude and'], shape=(2,), dtype=string)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h5U4hWxFVEWD",
        "outputId": "bb613d67-4b4d-440b-e5fb-3f14227fbfe7"
      },
      "source": [
        "# Set up tokenizer to generate Tensorflow dataset\n",
        "import os\n",
        "tokenizer = bert.tokenization.FullTokenizer(\n",
        "    vocab_file=os.path.join(gs_folder_bert, \"vocab.txt\"),\n",
        "     do_lower_case=True)\n",
        "\n",
        "print(\"Vocab size:\", len(tokenizer.vocab))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocab size: 30522\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BdA4R0o7JNkg"
      },
      "source": [
        "def encode_sentence(s, tokenizer):\n",
        "   tokens = list(tokenizer.tokenize(s))\n",
        "   tokens.append('[SEP]')\n",
        "   return tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "def bert_encode(trans_csv, tokenizer):\n",
        "  num_examples = len(trans_csv[\"transcript\"])\n",
        "\n",
        "  trans_sens = tf.ragged.constant([\n",
        "      encode_sentence(s, tokenizer)\n",
        "      for s in np.array(trans_csv[\"transcript\"])])\n",
        "  \n",
        "\n",
        "  cls = [tokenizer.convert_tokens_to_ids(['[CLS]'])]*trans_sens.shape[0]\n",
        "  input_word_ids = tf.concat([cls, trans_sens], axis=-1)\n",
        "\n",
        "  input_mask = tf.ones_like(input_word_ids).to_tensor()\n",
        "\n",
        "  type_cls = tf.zeros_like(cls)\n",
        "  type_s = tf.zeros_like(trans_sens)\n",
        "  input_type_ids = tf.concat(\n",
        "      [type_cls, type_s], axis=-1).to_tensor()\n",
        "\n",
        "  inputs = {\n",
        "      'input_word_ids': input_word_ids.to_tensor(),\n",
        "      'input_mask': input_mask,\n",
        "      'input_type_ids': input_type_ids}\n",
        "\n",
        "  return inputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ynXpI66GmRvv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21f3267b-415a-4d43-9675-ea66922d6068"
      },
      "source": [
        "import json\n",
        "import numpy as np\n",
        "sen_extractor_inputs = bert_encode(transcript_csv, tokenizer)\n",
        "sen_extractor_outputs = transcript_csv[\"importance\"]\n",
        "\n",
        "\n",
        "bert_config_file = os.path.join(gs_folder_bert, \"bert_config.json\")\n",
        "config_dict = json.loads(tf.io.gfile.GFile(bert_config_file).read())\n",
        "\n",
        "bert_config = bert.configs.BertConfig.from_dict(config_dict)\n",
        "\n",
        "\n",
        "bert_classifier, bert_encoder = bert.bert_models.classifier_model(\n",
        "    bert_config, num_labels=2)\n",
        "\n",
        "checkpoint = tf.train.Checkpoint(encoder=bert_encoder)\n",
        "checkpoint.read(\n",
        "    os.path.join(gs_folder_bert, 'bert_model.ckpt')).assert_consumed()\n",
        "\n",
        "# Set up epochs and steps\n",
        "epochs = 3\n",
        "batch_size = 32\n",
        "eval_batch_size = 32\n",
        "\n",
        "train_data_size = len(sen_extractor_outputs)\n",
        "steps_per_epoch = int(train_data_size / batch_size)\n",
        "num_train_steps = steps_per_epoch * epochs\n",
        "warmup_steps = int(epochs * train_data_size * 0.1 / batch_size)\n",
        "\n",
        "# creates an optimizer with learning rate schedule\n",
        "optimizer = nlp.optimization.create_optimizer(\n",
        "    2e-5, num_train_steps=num_train_steps, num_warmup_steps=warmup_steps)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/array_ops.py:5049: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CrjIt9H4luyw",
        "outputId": "cd395271-bd4f-4a5d-86ec-326bc6204e23"
      },
      "source": [
        "sen_extractor_input_test = bert_encode(transcript_csv.head(10), tokenizer)\n",
        "#bert_encoder(np.array(sen_extractor_input_test))\n",
        "bert_encoder.get_embedding_layer()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<official.nlp.keras_nlp.layers.on_device_embedding.OnDeviceEmbedding at 0x7f1dea79b390>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dAChgbTInspZ",
        "outputId": "8a7550af-e5fc-4e3b-afd3-1bf35ff1fb9d"
      },
      "source": [
        "metrics = [tf.keras.metrics.SparseCategoricalAccuracy('accuracy', dtype=tf.float32)]\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "bert_classifier.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss=loss,\n",
        "    metrics=metrics)\n",
        "\n",
        "bert_classifier.fit(\n",
        "      sen_extractor_inputs, sen_extractor_outputs,\n",
        "      batch_size=32,\n",
        "      epochs=10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "8/8 [==============================] - 83s 9s/step - loss: 0.5985 - accuracy: 0.6540\n",
            "Epoch 2/10\n",
            "8/8 [==============================] - 71s 9s/step - loss: 0.5963 - accuracy: 0.6498\n",
            "Epoch 3/10\n",
            "8/8 [==============================] - 71s 9s/step - loss: 0.6027 - accuracy: 0.6498\n",
            "Epoch 4/10\n",
            "8/8 [==============================] - 71s 9s/step - loss: 0.5972 - accuracy: 0.6582\n",
            "Epoch 5/10\n",
            "8/8 [==============================] - 72s 9s/step - loss: 0.6001 - accuracy: 0.6498\n",
            "Epoch 6/10\n",
            "8/8 [==============================] - 72s 9s/step - loss: 0.5900 - accuracy: 0.6624\n",
            "Epoch 7/10\n",
            "8/8 [==============================] - 72s 9s/step - loss: 0.5997 - accuracy: 0.6540\n",
            "Epoch 8/10\n",
            "8/8 [==============================] - 72s 9s/step - loss: 0.5958 - accuracy: 0.6582\n",
            "Epoch 9/10\n",
            "8/8 [==============================] - 72s 9s/step - loss: 0.5986 - accuracy: 0.6540\n",
            "Epoch 10/10\n",
            "8/8 [==============================] - 71s 9s/step - loss: 0.5968 - accuracy: 0.6624\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f548ba34e10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZWUTYYbVq74J",
        "outputId": "d46a0c0f-a248-49eb-c0aa-ceb23ed2ed8c"
      },
      "source": [
        "bert_classifier.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"bert_classifier_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_word_ids (InputLayer)     [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_mask (InputLayer)         [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_type_ids (InputLayer)     [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "bert_encoder_3 (BertEncoder)    [(None, None, 768),  109482240   input_word_ids[0][0]             \n",
            "                                                                 input_mask[0][0]                 \n",
            "                                                                 input_type_ids[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dropout_3 (Dropout)             (None, 768)          0           bert_encoder_3[0][1]             \n",
            "__________________________________________________________________________________________________\n",
            "sentence_prediction (Classifica (None, 2)            1538        dropout_3[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 109,483,778\n",
            "Trainable params: 109,483,778\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t71w5p-_aclO"
      },
      "source": [
        "## **Using Up BERT**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pCe5xUCipKa7",
        "outputId": "799b901a-fbf8-447f-9416-08485d011afd"
      },
      "source": [
        "test_sens = transcript_csv #{\"transcript\":[\"vector is a useful metric\", \"this is horizontal direction\", \"and is shifting 4 positive 4 in the\", \"vector so if someone tells you that\"], \"importance\":[1, 0, 1, 0]}\n",
        "test_inputs = bert_encode(test_sens, tokenizer)\n",
        "result = bert_classifier(test_inputs, training=False, )\n",
        "result = tf.argmax(result, axis=1).numpy()\n",
        "result\n",
        "\n",
        "shortened_transcript = {\"transcript\":[]} \n",
        "_ = [shortened_transcript[\"transcript\"].append(test_sens[\"transcript\"][i]) for i in range(len(result)) if result[i] == 1]\n",
        "\n",
        "shortened_transcript[\"transcript\"]\n",
        "percent_of_compression = (len(test_sens[\"transcript\"]) - len(shortened_transcript[\"transcript\"]))/len(test_sens[\"transcript\"])\n",
        "print(percent_of_compression*100, \"%: compression\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8438818565400843 %: compression\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "id": "IDvgWSfHerkM",
        "outputId": "2d8d8587-71d2-4351-9175-1bc595b79896"
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "def analyse_transcript(trans_csv):\n",
        "  x = np.array(trans_csv[\"importance\"])\n",
        "\n",
        "# Creating histogram\n",
        "  fig, ax = plt.subplots(figsize =(10, 7))\n",
        "  ax.hist(x)\n",
        "\n",
        "  # Show plot\n",
        "  plt.xlim(0,1)\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def display_transcript(pfc):\n",
        "  if pfc >=0.2:\n",
        "    return shortened_transcript[\"transcript\"]\n",
        "  else:\n",
        "    return \" \"\n",
        "analyse_transcript(pd.read_csv(\"/content/drive/MyDrive/Short Talks BERT/train data csv/khan_academy_01.csv\"))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJGi-Ua4adp8"
      },
      "source": [
        "## **BERT Encoder Usage-2**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "27z9DaEbcoEg",
        "outputId": "3bc96524-09f2-4590-ef4e-e42704253d97"
      },
      "source": [
        "!pip install bert-for-tf2\n",
        "!pip install tensorflow==2.0\n",
        "!pip install tensorflow_hub\n",
        "!pip install sentencepiece"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting bert-for-tf2\n",
            "  Downloading bert-for-tf2-0.14.9.tar.gz (41 kB)\n",
            "\u001b[?25l\r\u001b[K     |████████                        | 10 kB 25.0 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 20 kB 26.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 30 kB 12.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 40 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 41 kB 131 kB/s \n",
            "\u001b[?25hCollecting py-params>=0.9.6\n",
            "  Downloading py-params-0.10.2.tar.gz (7.4 kB)\n",
            "Collecting params-flow>=0.8.0\n",
            "  Downloading params-flow-0.8.2.tar.gz (22 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (1.19.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (4.62.0)\n",
            "Building wheels for collected packages: bert-for-tf2, params-flow, py-params\n",
            "  Building wheel for bert-for-tf2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bert-for-tf2: filename=bert_for_tf2-0.14.9-py3-none-any.whl size=30534 sha256=177495aad2121fdb0412f8bd02f531fb1e22759822b6f346a786cee8136d1cff\n",
            "  Stored in directory: /root/.cache/pip/wheels/47/b6/e5/8c76ec779f54bc5c2f1b57d2200bb9c77616da83873e8acb53\n",
            "  Building wheel for params-flow (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for params-flow: filename=params_flow-0.8.2-py3-none-any.whl size=19473 sha256=1084b9b06b4f8b172ede818052d59ec09db6c9fce2342b8c4031a11edd6d6931\n",
            "  Stored in directory: /root/.cache/pip/wheels/0e/fc/d2/a44fff33af0f233d7def6e7de413006d57c10e10ad736fe8f5\n",
            "  Building wheel for py-params (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for py-params: filename=py_params-0.10.2-py3-none-any.whl size=7912 sha256=80a5b11a1d3383a660aa7e7470db8b0793edb5349a249627819f41a4197954e2\n",
            "  Stored in directory: /root/.cache/pip/wheels/e1/11/67/33cc51bbee127cb8fb2ba549cd29109b2f22da43ddf9969716\n",
            "Successfully built bert-for-tf2 params-flow py-params\n",
            "Installing collected packages: py-params, params-flow, bert-for-tf2\n",
            "Successfully installed bert-for-tf2-0.14.9 params-flow-0.8.2 py-params-0.10.2\n",
            "Collecting tensorflow==2.0\n",
            "  Downloading tensorflow-2.0.0-cp37-cp37m-manylinux2010_x86_64.whl (86.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 86.3 MB 11 kB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0) (0.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0) (1.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0) (1.12.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0) (1.15.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0) (1.19.5)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0) (0.8.1)\n",
            "Collecting keras-applications>=1.0.8\n",
            "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 6.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0) (1.1.2)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0) (1.39.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0) (0.2.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0) (3.17.3)\n",
            "Collecting tensorboard<2.1.0,>=2.0.0\n",
            "  Downloading tensorboard-2.0.2-py3-none-any.whl (3.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 39.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0) (0.37.0)\n",
            "Collecting gast==0.2.2\n",
            "  Downloading gast-0.2.2.tar.gz (10 kB)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0) (3.3.0)\n",
            "Collecting tensorflow-estimator<2.1.0,>=2.0.0\n",
            "  Downloading tensorflow_estimator-2.0.1-py2.py3-none-any.whl (449 kB)\n",
            "\u001b[K     |████████████████████████████████| 449 kB 43.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow==2.0) (3.1.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (0.4.5)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (3.3.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (2.23.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (1.0.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (1.34.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (57.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (4.7.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (4.2.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (4.6.3)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (0.4.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (2021.5.30)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (3.1.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.8->tensorflow==2.0) (1.5.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (3.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (3.7.4.3)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-py3-none-any.whl size=7554 sha256=d08dd3a051465fdcb7ce454e7ed6a547b4f23a5ed5026ea11a406cbf7b31b5d2\n",
            "  Stored in directory: /root/.cache/pip/wheels/21/7f/02/420f32a803f7d0967b48dd823da3f558c5166991bfd204eef3\n",
            "Successfully built gast\n",
            "Installing collected packages: tensorflow-estimator, tensorboard, keras-applications, gast, tensorflow\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.6.0\n",
            "    Uninstalling tensorflow-estimator-2.6.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.6.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.6.0\n",
            "    Uninstalling tensorboard-2.6.0:\n",
            "      Successfully uninstalled tensorboard-2.6.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.4.0\n",
            "    Uninstalling gast-0.4.0:\n",
            "      Successfully uninstalled gast-0.4.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.6.0\n",
            "    Uninstalling tensorflow-2.6.0:\n",
            "      Successfully uninstalled tensorflow-2.6.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-probability 0.13.0 requires gast>=0.3.2, but you have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "Successfully installed gast-0.2.2 keras-applications-1.0.8 tensorboard-2.0.2 tensorflow-2.0.0 tensorflow-estimator-2.0.1\n",
            "Requirement already satisfied: tensorflow_hub in /usr/local/lib/python3.7/dist-packages (0.12.0)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_hub) (1.19.5)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_hub) (3.17.3)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.8.0->tensorflow_hub) (1.15.0)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 5.3 MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.96\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKaskVp2aj3M"
      },
      "source": [
        "import tensorflow_hub as hub\n",
        "import tensorflow as tf\n",
        "import bert\n",
        "import os\n",
        "FullTokenizer = bert.bert_tokenization.FullTokenizer\n",
        "from tensorflow.keras.models import Model       # Keras is the new high level API for TensorFlow\n",
        "import math"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S8a0ytbdeOye"
      },
      "source": [
        "def make_bert_encoder():\n",
        "  max_seq_length = 128  # Your choice here.\n",
        "  input_word_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
        "                                        name=\"input_word_ids\")\n",
        "  input_mask = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
        "                                    name=\"input_mask\")\n",
        "  segment_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
        "                                      name=\"segment_ids\")\n",
        "  bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\n",
        "                              trainable=True)\n",
        "  pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n",
        "\n",
        "  model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=[pooled_output, sequence_output])\n",
        "  return model\n",
        "  \n",
        "def get_masks(tokens, max_seq_length):\n",
        "    \"\"\"Mask for padding\"\"\"\n",
        "    if len(tokens)>max_seq_length:\n",
        "        raise IndexError(\"Token length more than max seq length!\")\n",
        "    return [1]*len(tokens) + [0] * (max_seq_length - len(tokens))\n",
        "\n",
        "\n",
        "def get_segments(tokens, max_seq_length):\n",
        "    \"\"\"Segments: 0 for the first sequence, 1 for the second\"\"\"\n",
        "    if len(tokens)>max_seq_length:\n",
        "        raise IndexError(\"Token length more than max seq length!\")\n",
        "    segments = []\n",
        "    current_segment_id = 0\n",
        "    for token in tokens:\n",
        "        segments.append(current_segment_id)\n",
        "        if token == \"[SEP]\":\n",
        "            current_segment_id = 1\n",
        "    return segments + [0] * (max_seq_length - len(tokens))\n",
        "\n",
        "\n",
        "def get_ids(tokens, tokenizer, max_seq_length):\n",
        "    \"\"\"Token ids from Tokenizer vocab\"\"\"\n",
        "    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "    input_ids = token_ids + [0] * (max_seq_length-len(token_ids))\n",
        "    return input_ids"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cjqmosqec-XD",
        "outputId": "fda8c247-283d-4ab8-8775-585312ccff21"
      },
      "source": [
        "model_bert_encoder = make_bert_encoder()\n",
        "\n",
        "model_bert_encoder.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_3\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_word_ids (InputLayer)     [(None, 128)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_mask (InputLayer)         [(None, 128)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "segment_ids (InputLayer)        [(None, 128)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "keras_layer_3 (KerasLayer)      [(None, 768), (None, 109482241   input_word_ids[0][0]             \n",
            "                                                                 input_mask[0][0]                 \n",
            "                                                                 segment_ids[0][0]                \n",
            "==================================================================================================\n",
            "Total params: 109,482,241\n",
            "Trainable params: 109,482,240\n",
            "Non-trainable params: 1\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 911
        },
        "id": "pmXVCNOseX1s",
        "outputId": "6f752da4-8188-4a99-a9ff-54a78ab8c360"
      },
      "source": [
        "max_seq_length = 128  # Your choice here.\n",
        "input_word_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
        "                                        name=\"input_word_ids\")\n",
        "input_mask = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
        "                                    name=\"input_mask\")\n",
        "segment_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
        "                                      name=\"segment_ids\")\n",
        "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\n",
        "                              trainable=True)\n",
        "pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n",
        "\n",
        "model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=[pooled_output, sequence_output])\n",
        "\n",
        "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
        "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
        "tokenizer = FullTokenizer(vocab_file, do_lower_case)\n",
        "\n",
        "s = \"This is a nice sentence.\"\n",
        "stokens = tokenizer.tokenize(s)\n",
        "stokens = [\"[CLS]\"] + stokens + [\"[SEP]\"]\n",
        "\n",
        "input_ids = get_ids(stokens, tokenizer, max_seq_length)\n",
        "input_masks = get_masks(stokens, max_seq_length)\n",
        "input_segments = get_segments(stokens, max_seq_length)\n",
        "print(stokens)\n",
        "print(input_ids)\n",
        "print(input_masks)\n",
        "print(input_segments)\n",
        "pool_embs, all_embs = model.predict([[input_ids],[input_masks],[input_segments]])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['[CLS]', 'this', 'is', 'a', 'nice', 'sentence', '.', '[SEP]']\n",
            "[101, 2023, 2003, 1037, 3835, 6251, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-cca8102813ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_segments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mpool_embs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_embs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_masks\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_segments\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1569 predict_function  *\n        return step_function(self, iterator)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1559 step_function  **\n        \n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:1285 run\n        \n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:2833 call_for_each_replica\n        \n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:3608 _call_for_each_replica\n        \n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1552 run_step  **\n        \n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1525 predict_step\n        \n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer.py:1013 __call__\n        \n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/input_spec.py:203 assert_input_compatibility\n        \n\n    ValueError: Layer model_7 expects 3 input(s), but it received 1 input tensors. Inputs received: [<tf.Tensor 'IteratorGetNext:0' shape=(None, 1, 128) dtype=int64>]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zVxR8v6fu1y"
      },
      "source": [
        "# **NEVER MIND THE STUFF BELOW**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F45E_O4C5Y6i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c447e0e8-7747-45e2-9bcf-5d64055f9803"
      },
      "source": [
        "#JUST RUN THIS CELL\n",
        "\n",
        "import copy\n",
        "import nltk\n",
        "import numpy as np\n",
        "import string\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from nltk.corpus import stopwords\n",
        "from fuzzywuzzy import process\n",
        "from fuzzywuzzy import fuzz\n",
        "from scipy import stats\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def chunk(sourceFile, wordsPerLine = None, endLineAt = None ):\n",
        "  fi = open(sourceFile,\"r+\")\n",
        "  text = fi.read()\n",
        "  text = text.replace(\"\\n\",\"\")\n",
        "\n",
        "  \n",
        "\n",
        "  if wordsPerLine != None :\n",
        "   text = text.split()\n",
        "   for words in range(1,len(text)+1):\n",
        "      if words%3 == 0:\n",
        "        text[words-1] = text[words-1] + \"\\n\" \n",
        "   fi.seek(0)\n",
        "   fi.write(\" \".join(text))\n",
        "  if endLineAt != None :\n",
        "    \n",
        "    for words in endLineAt :\n",
        "      text = text.split(words)\n",
        "      text = \"\\n\".join(text)\n",
        "      \n",
        "\n",
        "    fi.seek(0)\n",
        "    fi.write(text)\n",
        "  \n",
        "  fi.close()\n",
        "  return\n",
        "\n",
        "def getKey(D,val): \n",
        "    for key, value in D.items(): \n",
        "         if val == value: \n",
        "             return key \n",
        "    return -1\n",
        "    \n",
        "def completeFiltering(singleStringTxt, multiLineTxt, limitOnFreq, limitOnDataW = 10000):\n",
        "  wholeText = singleStringTxt\n",
        "  cleansed = wholeText.split()[:limitOnDataW]\n",
        "  table = str.maketrans(\"\",\"\",string.punctuation)\n",
        "  cleansed = [w.translate(table) for w in cleansed]\n",
        "  patched = \" \".join(cleansed)\n",
        "  cleansed = patched.split()\n",
        "  cleansed = [words for words in cleansed if not words.lower() in stopwords.words()]\n",
        "  \n",
        "  cleansedTxt = \" \".join(cleansed)\n",
        "\n",
        "  wholeText = [cleansedTxt]\n",
        "  lineWiseText = multiLineTxt\n",
        "\n",
        "  # list of text documents\n",
        "  # create the transform\n",
        "  vectorizer1 = CountVectorizer()\n",
        "  vectorizer2 = CountVectorizer()\n",
        "  # tokenize and build vocab\n",
        "  vectorizer1.fit(wholeText)\n",
        "  vectorizer2.fit(lineWiseText)\n",
        "\n",
        "  # summarize\n",
        "  wToInd1 = vectorizer1.vocabulary_\n",
        "  wToInd2 = vectorizer2.vocabulary_\n",
        "  # encode document\n",
        "  vector1 = vectorizer1.transform(wholeText)\n",
        "  vector2 = vectorizer2.transform(lineWiseText)\n",
        "  # summarize encoded vector\n",
        "  v1 = vector1.toarray()\n",
        "  v2 = vector2.toarray()\n",
        "  \n",
        "  \n",
        "  finalCount = np.sum(v1,axis = 0,keepdims = False)\n",
        "  \n",
        "  countDict1 = dict()\n",
        "  \n",
        "  countDict2 = dict()\n",
        "  priorities2 = dict()\n",
        "  for ind in range(len(finalCount)):\n",
        "    if finalCount[ind] >=limitOnFreq :\n",
        "      countDict1[getKey(wToInd1,ind)] = finalCount[ind]\n",
        "  \n",
        "  for lines in range(v2.shape[0]):\n",
        "    countDict = dict()\n",
        "    for ind in range(v2.shape[1]):\n",
        "      if v2[lines][ind] >=limitOnFreq :\n",
        "        countDict[getKey(wToInd2,ind)] = v2[lines][ind]\n",
        "    \n",
        "    priorities = sorted(countDict,key=countDict.get,reverse=True)\n",
        "    \n",
        "    countDict2[str(lines+1)] = countDict\n",
        "    priorities2[str(lines+1)] = priorities\n",
        "\n",
        "  contentWords = superImportant(\"Apple Inc\")\n",
        "  countDict1, misMatch = changePriorities(countDict1, contentWords)\n",
        "  print(\"These many got mismatched : \", misMatch)\n",
        "  \n",
        "  priorities1 = sorted(countDict1,key=countDict1.get,reverse=True)\n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "  return priorities1, priorities2, countDict1, countDict2\n",
        "\n",
        "def fuzzyWayCondense(fileSource, priorities1, priorities2, prioritiesMap1, prioritiesMap2, limitOnLines = 3, limitOnDataL = 100, method = \"Frequency\", printLineScores = False):\n",
        "  \n",
        "  if method == \"Frequency\":\n",
        "    priorities = priorities1\n",
        "    prioritiesMap = prioritiesMap1\n",
        "  elif method == \"TF-IDF\":\n",
        "    priorities = priorities1\n",
        "    prioritiesMap = prioritiesMap1\n",
        "    prioritiesMapext = prioritiesMap2\n",
        "    includeTFIDF = np.zeros((limitOnDataL,len(priorities)))\n",
        "\n",
        "  fi = open(fileSource,\"r\")\n",
        "  include = np.zeros((limitOnDataL,len(priorities)))\n",
        "  \n",
        "  wholeLines = fi.readlines()[:limitOnDataL]\n",
        "  maintain = dict()\n",
        "  \n",
        "  for lines in range(1,limitOnDataL +1):\n",
        "    maintain[str(lines)] = []\n",
        "  \n",
        "  fi.close()\n",
        "  \n",
        "  for words in priorities:\n",
        "    options = process.extract(words,wholeLines,limit = limitOnDataL)\n",
        "    for line,score in options:\n",
        "      \n",
        "      if (words in line.split()) and method == \"Frequency\":\n",
        "        maintain[str(wholeLines.index(line)+1)].append(words)\n",
        "        include[wholeLines.index(line)][priorities.index(words)] = score*prioritiesMap[words]\n",
        "      elif ( words in line.split() ) and method == \"TF-IDF\":\n",
        "        maintain[str(wholeLines.index(line)+1)].append(words)\n",
        "        includeTFIDF[wholeLines.index(line)][priorities.index(words)] = prioritiesMapext[str(wholeLines.index(line)+1)][words]*prioritiesMap[words]\n",
        "  \n",
        "  if method == \"TF-IDF\":\n",
        "    \n",
        "    includeTFIDF = list(np.sum(includeTFIDF,axis=0))\n",
        "    \n",
        "    for words in priorities:\n",
        "      options = process.extract(words,wholeLines,limit = limitOnDataL)\n",
        "      for line,score in options:\n",
        "      \n",
        "        if (words in line.split()) :\n",
        "          \n",
        "          include[wholeLines.index(line)][priorities.index(words)] = score*includeTFIDF[priorities.index(words)]\n",
        "\n",
        "  for lines in range(1,limitOnDataL +1):\n",
        "     maintain[str(lines)] = set(maintain[str(lines)] )\n",
        "\n",
        "  include = list(np.sum(include,axis =1))\n",
        "  includeTemp = np.array(copy.deepcopy(include))\n",
        "  \n",
        "  if printLineScores == True :\n",
        "    print(\"\\nThe Scores of the Sentences from 1 to\", limitOnDataL,\" are as follows \\n\", include)\n",
        "    print(\"\\nThe Key Words Per Line for all the lines are : \\n\", maintain)\n",
        " \n",
        "  \n",
        "  \n",
        "  condensedLines = []\n",
        "  condensedLinesIndices = []\n",
        "  if limitOnLines != \"NormSTDPick\" :\n",
        "    includeTemp = (np.sort(includeTemp))[::-1]\n",
        "    for i in range(limitOnLines) :\n",
        "      condensedLines.append(wholeLines[include.index(includeTemp[i])])\n",
        "      condensedLinesIndices.append(include.index(includeTemp[i]) + 1)\n",
        "      include[include.index(includeTemp[i])] = -1\n",
        "  else :\n",
        "    includeTemp = np.array([(value >= np.percentile(includeTemp, 90))for value in includeTemp]).astype(int)\n",
        "    includeTemp = np.reshape(np.argwhere(includeTemp), (-1,)) + 1\n",
        "    condensedLines = [wholeLines[i-1] for i in includeTemp]\n",
        "    condensedLinesIndices = includeTemp\n",
        "\n",
        "  condensedText = \" \".join(condensedLines)\n",
        "\n",
        "  return condensedText, condensedLines, condensedLinesIndices\n",
        "\n",
        "         \n",
        "        \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4Dbs1p0hmKS"
      },
      "source": [
        "import json\n",
        "T1path = \"/content/drive/My Drive/AppleTranscript1.txt\" #\"/content/drive/My Drive/transcript1\"\n",
        "T2path = \"/content/drive/My Drive/AppleTranscript2.txt\" #\"/content/drive/My Drive/transcript2\"\n",
        "T3path = \"/content/drive/My Drive/AppleTranscript3.txt\" #\"/content/drive/My Drive/transcript3\"\n",
        "T4path = \"/content/drive/My Drive/AppleTranscript4.txt\" #\"/content/drive/My Drive/transcript4\"\n",
        "Tpath = \"/content/drive/My Drive/entireAppleTranscript.txt\" #\"/content/drive/My Drive/entireTranscript.txt\"\n",
        "\n",
        "transcript = \"\" \n",
        "tpt1 = open(T1path,\"r\")\n",
        "T1 = tpt1.read()\n",
        "T1dict = eval(T1)\n",
        "tpt1.close()\n",
        "\n",
        "tpt2 = open(T2path,\"r\")\n",
        "T2 = tpt2.read()\n",
        "T2dict = eval(T2)\n",
        "tpt2.close()\n",
        "\n",
        "tpt3 = open(T3path,\"r\")\n",
        "T3 = tpt3.read()\n",
        "T3dict = eval(T3)\n",
        "#T3dict = json.loads(T3dict.decode(\"utf-8\"))\n",
        "tpt3.close()\n",
        "\n",
        "tpt4 = open(T4path,\"r\")\n",
        "T4 = tpt4.read()\n",
        "T4dict = eval(T4)\n",
        "#T4dict = json.loads(T4dict.decode(\"utf-8\"))\n",
        "tpt4.close()\n",
        "\n",
        "transcript += T1dict[\"response\"][\"transcript\"]\n",
        "transcript += T2dict[\"response\"][\"transcript\"]\n",
        "transcript += T3dict[\"response\"][\"transcript\"]\n",
        "transcript += T4dict[\"response\"][\"transcript\"]\n",
        "\n",
        "tpt = open(Tpath,\"w\")\n",
        "tpt.write(transcript)\n",
        "tpt.close()\n",
        "\n",
        "#tpt  = open(Tpath,\"r\")\n",
        "Tdicts = [T1dict,T2dict,T3dict,T4dict]\n",
        "#print(tpt.read())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5VfFjOBd4_E4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6bffb3b4-5c19-4b65-b815-277e6ccdcc99"
      },
      "source": [
        "#RUN THIS CELL WITH SPECIFIED PATH TO LOAD ALL THE TEXT FILE AS STRING INTO \"wholeText\" AND TEXT FILE AS LINES INTO \"lineWiseText\"\n",
        "\n",
        "#path = \"/content/drive/My Drive/TedTranscript.txt\" #Path of the file from the drive\n",
        "path = Tpath # \"/content/drive/My Drive/entireTranscript.txt\"\n",
        "\n",
        "chunk(path,endLineAt=[\".\",\"?\"])               \n",
        "\n",
        "fi = open(path,\"r\")\n",
        "wholeText = fi.read()\n",
        "fi.seek(0)\n",
        "totalWords = len((fi.read()).split())\n",
        "fi.seek(0)\n",
        "totalLines = len(fi.readlines())\n",
        "fi.seek(0)\n",
        "lineWiseText = fi.readlines()\n",
        "fi.close()\n",
        "\n",
        "print(\"Total Lines present in the Source File is : \", totalLines)\n",
        "print(\"Total Words present in the source File is : \", totalWords)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Lines present in the Source File is :  289\n",
            "Total Words present in the source File is :  6076\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2LAcVjAP5puy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01e0ff2c-6bd3-4455-91db-f342044f3050"
      },
      "source": [
        "# \"completeFiltering\" func takes \"wholeText\", \"lineWiseText\", \"limitOnFreq\" (let this be unchanged), \"limitOnDataW\" (this equals the \"totalWords\" in above cell)\n",
        "# \"completeFiltering\" func returns priorities1,2 and countDict1,2 which are used more for internal purposes so I'm hiding these outputs\n",
        "priorities1, priorities2, countDict1, countDict2 = completeFiltering(wholeText, lineWiseText,limitOnFreq = 1, limitOnDataW=totalWords)\n",
        "\n",
        "#print(\"\\nTop Prior Words of 10K Words data : \", priorities1)\n",
        "#print(\"\\nTop Prior Words of every Line data : \", priorities2)\n",
        "\n",
        "# \"fuzzyWayCondense\" func takes \"path\", \"priorities1,2\", \"countDict1,2\", \"limitOnLines\" (this is can be anything <= \"totalLines\"), \"limitonDataL\"(this equals the \"totalLines\" in above cell), \"method\" (let it be unchanged), \"printLineScores\"(let it be False setting it to True just prints scores which are of no use to u)\n",
        "# \"fuzzyWayCondense\" func returns \"condensedText\"(optional use to u), \"condensedLines\"(optional use to u just gives list of line strings) ,\"condensedLinesIndices1(u might need this)\"\n",
        "condensedText, condensedLines, condensedLinesIndices1 = fuzzyWayCondense(path,priorities1, priorities2,countDict1, countDict2, limitOnLines=\"NormSTDPick\", limitOnDataL = totalLines, method = \"TF-IDF\", printLineScores=False)\n",
        "print(\"\\nThis is the TF-IDF Way : \\n\")\n",
        "print(\"\\nThe Original Lines which made thorugh the filtering process  are the line numbers : \\n\", condensedLinesIndices1)\n",
        "#print(\"\\nOverall the condensed Text : \\n\", condensedText)\n",
        "\n",
        "# \"fuzzyWayCondense\" func takes \"path\", \"priorities1,2\", \"countDict1,2\", \"limitOnLines\" (this is can be anything <= \"totalLines\"), \"limitonDataL\"(this equals the \"totalLines\" in above cell), \"method\" (let it be unchanged), \"printLineScores\"(let it be False setting it to True just prints scores which are of no use to u)\n",
        "# \"fuzzyWayCondense\" func returns \"condensedText\"(optional use to u), \"condensedLines\"(optional use to u just gives list of line strings) ,\"condensedLinesIndices2(u might need this)\"\n",
        "#condensedText, condensedLines, condensedLinesIndices2 = fuzzyWayCondense(path,priorities1, priorities2,countDict1, countDict2, limitOnLines=\"NormSTDPick\", limitOnDataL = totalLines, method = \"Frequency\", printLineScores=False)\n",
        "#print(\"\\nThis is the Frequency Way : \\n\")\n",
        "#print(\"\\nThe Original Lines which made thorugh the filtering process  are the line numbers : \\n\", condensedLinesIndices2)\n",
        "#print(\"\\nOverall the condensed Text : \\n\", condensedText)\n",
        "\n",
        "# \"finalSet\" gives union of results of both methods\n",
        "#finalSet =  set(condensedLinesIndices1).union(set(condensedLinesIndices2))\n",
        "finalSet =  set(condensedLinesIndices1)\n",
        "print(\"\\nConsider these lines as Important : \", finalSet )\n",
        "print(\"\\nPercentage of Condensation of initial Text is : {:.4f}%\".format(((totalLines - len(finalSet))/totalLines)*100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/wikipedia/wikipedia.py:389: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
            "\n",
            "The code that caused this warning is on line 389 of the file /usr/local/lib/python3.6/dist-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
            "\n",
            "  lis = BeautifulSoup(html).find_all('li')\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "These many got mismatched :  570\n",
            "\n",
            "This is the TF-IDF Way : \n",
            "\n",
            "\n",
            "The Original Lines which made thorugh the filtering process  are the line numbers : \n",
            " [ 40  43  46  52  56  62  67  89  90  99 146 150 152 159 160 166 184 185\n",
            " 186 189 194 196 198 199 211 215 219 221 228]\n",
            "\n",
            "Consider these lines as Important :  {146, 150, 152, 159, 160, 166, 40, 43, 46, 52, 184, 185, 186, 56, 189, 62, 194, 67, 196, 198, 199, 211, 215, 89, 90, 219, 221, 99, 228}\n",
            "\n",
            "Percentage of Condensation of initial Text is : 89.9654%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TafGsj8NYTbb"
      },
      "source": [
        "\n",
        "Jsonpath = \"/content/drive/My Drive/entireAppleJason.txt\" #\"/content/drive/My Drive/entireJasonObj.txt\"\n",
        "\n",
        "correction = 0\n",
        "for transcripts in range(4):\n",
        "    for times in Tdicts[transcripts][\"response\"][\"words\"]:\n",
        "\n",
        "      times[\"start\"] = times[\"start\"] + correction\n",
        "      times[\"end\"] = times[\"end\"] + correction\n",
        "      mark = times[\"end\"]\n",
        "      \n",
        "    correction =   mark\n",
        "\n",
        "jsonObjs = {\"response\" : dict()}\n",
        "jsonObjs[\"response\"][\"words\"] = T1dict[\"response\"][\"words\"] + T2dict[\"response\"][\"words\"] + T3dict[\"response\"][\"words\"] + T4dict[\"response\"][\"words\"]\n",
        "jsonObjs = str(jsonObjs)\n",
        "json = open(Jsonpath,\"w\")\n",
        "json.write(jsonObjs)\n",
        "json.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jauVyEe_8zDw"
      },
      "source": [
        "\n",
        "pathr = Jsonpath #\"/content/drive/My Drive/tedxtJasonObject\" # PATH TO THE TEXT FILE CONTAINING JASON OBJECT (DICTIONARY OF REQUEST ID, RESPONSE, TIMESTAMPS OF WORDS ....)\n",
        "fi = open(pathr,\"r\")\n",
        "fullDataDict = eval(fi.read())\n",
        "def convert(seconds): \n",
        "    seconds = seconds % (24 * 3600) \n",
        "    hour = seconds // 3600\n",
        "    seconds %= 3600\n",
        "    minutes = seconds // 60\n",
        "    seconds %= 60\n",
        "      \n",
        "    return \"%d:%02d:%02d\" % (hour, minutes, seconds) \n",
        "\n",
        "wholeWords = wholeText.split()\n",
        "wholeLines = lineWiseText\n",
        "prevWordsCount = dict()\n",
        "wds = 0\n",
        "for sen in wholeLines:\n",
        "     prevWordsCount[str(wholeLines.index(sen)+1)] = wds\n",
        "     wds += len(sen.split())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8xsk3tR9JQZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "483adedb-aa4c-4b18-db90-2824a93cb500"
      },
      "source": [
        "\n",
        "timeStamps = dict()\n",
        "\n",
        "for line in finalSet:\n",
        "     \n",
        "     try:\n",
        "      startInd = prevWordsCount[str(line)]\n",
        "      endInd = prevWordsCount[str(line+1)] -1\n",
        "     # USE \"convert\" if u need the output time stamps to be in HH:MM:SS if not remove \"convert\"\n",
        "      timeRange = (fullDataDict[\"response\"][\"words\"][startInd][\"start\"], fullDataDict[\"response\"][\"words\"][endInd][\"end\"])\n",
        "      timeStamps[int(line)] = timeRange\n",
        "      count =1\n",
        "     except :\n",
        "      print(\"\")\n",
        "\n",
        "c = 0\n",
        "Stamps = []\n",
        "for line in sorted(timeStamps):\n",
        "  c+=1\n",
        "  Stamps.append(timeStamps[line])\n",
        "print(Stamps)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(385.8, 414.6), (440.6, 483.5), (501.4, 532.1), (561.0, 601.3), (634.0999999999999, 661.3), (695.0, 729.8), (742.3, 747.0), (873.9, 902.5), (902.5, 913.6999999999999), (976.8, 979.9), (1381.1999999999998, 1397.1), (1412.5, 1423.6999999999998), (1431.6999999999998, 1448.8), (1489.1999999999998, 1523.0), (1523.0, 1562.0), (1607.3, 1647.6), (1795.8, 1812.3999999999999), (1812.3999999999999, 1817.5), (1817.5, 1834.5), (1873.8999999999999, 1891.1), (1930.5, 1962.3), (1968.8999999999999, 1973.6999999999998), (1978.1999999999998, 1983.6999999999998), (1983.6999999999998, 2003.8), (2092.1, 2092.9), (2133.8, 2151.2), (2174.6, 2197.6), (2202.2, 2205.5), (2263.4, 2282.4)]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}