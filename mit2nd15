Behind doing this morning?  I'm going to start with my New England.  Patriots example, so here are some data points about current.  Patriots players and I got two kinds receivers and I have alignment in each ones. This label by the name, the height and inches in the weight in pounds of each. If I plot those on a two dimensional plot, This is what I get, Okay, No big deal when we're trying to do.  I'm trying to learn are their characteristics that distinguish the two classes from one another and in the unlabeled case, All I have are just a set of examples, so what I want to do is decide what makes two players similar with the goal of seeing. Can I separate this distribution into two or more natural groups similar is a distance measure.  Just how do I take two examples with values or features associated with decide?  How far apart are they case?  The simple way to do, It is to say if I know that there are at least K groups are in this case.  I'm going to tell you.  There are two different groups there.  How could I decide how best to Cluster things together so that all the examples in one group are close to each other, all the examples in the other group or close to each other and they're reasonably far apart.  There are many ways to do it.  I'm going to show you one so very standard weight, and it works basically as follows the all I know is that there are two groups there.  I'm going to start by just picking two examples as my exemplars pick up at random. Actually random is not great.  I want to pick to close each other.  I'm going to try and pick them far, but I think two examples as my exemplars, and for all the other examples in the training data, I say, which one is the closest to try and do is create clusters with the property that the distance is between all of the examples that cluster or small, the average distance to small and see if I can find clusters against the average distance for both cluster of small as possible, This algorithm works by picking two examples clustering all the other examples by simply saying, put it in the group, which it, which is closest to that example.  I'm going to find the median element of that group, not mean, but meeting what's the one closest to the center and treat those as my exemplars and repeat the process and I'll just do it either some number of times until I don't get any change in the process, so it's clustering based on distance and we'll come back to distance in a second, so here's what I have with my football players. If I just did this based on. Wait, there's the natural dividing line, and it kind of makes sense, right, These three are obviously clustered.  You can assist on this axis. They're all down here.  These seven are at a different place. There's a natural dividing line there If I were to do it based on height, not as clean.  This is what my older than came up with is the best dividing line here, meaning that these four game, just based on this axis are close together.  These six are close together, but it's not nearly as clean, and that's part of the usual Look at is, how do I find the best clusters? If I use both height and weight? I get that cuz actually kind of nice, right.  Those three cluster together, they're near each other in terms of just distance in the plane, those seven or near each other. There's a nice natural dividing line through here in the fact that gives me a classifier.  This line is the equidistant line between the centers of those two clusters, meaning any point along.  This line is the same distance to the center that group as it is to that group. It's so any new example.  If it's a bubble line, I would say gets that label.  If it's below the line gets that label in the second will come back to look at.  How do we measure?  The distance is, but the idea here is pretty simple.  I want to find groupings near each other and far apart from the other group. Now suppose I actually knew the labels on these are play.  Earth Ocean called them objects are people these players?  These are the receivers.  Those are the lineman for those of you are football pants pants.  You can figure out, right.  Those are the two tight ends. They're much bigger.  I think that's been it, and that's Gronk. If you're really a big Patriots fan. But those are tight ends.  Those are wide receivers.  I'm going to come back in a sec, but there are the labels now. What I want to do is say if I could take advantage of knowing the labels, How would I divide these groups up and that's kind of easy to see basic idea in this case is if I've got labeled groups in that future space, What I want to do is find a subsurface that naturally divides that space now subsurface is a fancy word, it says in the two dimensional case.  I don't want to know what's the best line. If I can find a single line that separates all the examples with one label from all the examples of the second label will see that if the examples are well separated.  This is easy to do and it's great. But in some cases, it's going to be more complicated because some of the examples may be very close to one another. And that's going to raise the problem that you saw the last lecture.  I want to avoid overfitting.  I don't want to create a really complicated surface to separate things, and so we may have to tolerate a few incorrectly labeled things if we can pull it up and as you already figured out in this case with the label data, there's the best fitting line right there. Anybody over 280 lb is going to be a great lineman.  Anybody under 280 lb is more likely to be a receiver. Okay, so I got two different ways of trying to think about doing this label and we come back to both of them in a second now, suppose?  I added some new data.  I want to label new instances now.  These are actually players have a different position.  These are running backs. But I say all I know about is receivers and linemen.  I get these two new data points.  I'd like to know. Are they more likely to be a receiver or lineman? And there's the data for these two, gentlemen. So if I go back to now plotting them.  Oh, you noticed one of the the issue. So there are my line when the red ones are my receivers.  The two black dots are the two running backs. Noticed right here.  I'll be really hard to separate those two examples from one another. They are so close to each other, and that's going to be one of the things we have to trade off, But if I think about using what I learned as a classifier with unlabeled data, there were my two clusters.  Now you see how?  I've got an interesting example.  This new example I would say is clearly more like a receiver than a lineman For that one. There unclear almost exactly lies along that dividing line between those two clusters and that would either say.  I want to rethink the clustering, or I want to say you know what as I know.  Maybe there aren't two clusters here.  Maybe there are three, and I want to classify them a little differently, so I'll come back to that on the other hand. If I used the label data, there was my dividing line.  This is really easy!  Both of those new examples are clearly below the dividing line. They are clearly examples that I would categorize as being more like receivers than they are like lineman. And I know it's a football example if you don't like football pick another example, but you got the sense of why I can use the data in the label case in the unlabeled case to come up with different ways of building the Clusters.  So what we're going to do over the next two and a half lectures is look at. How can we write code to learn that way of separating things out where to learn model is based on unlabeled data. That's the case, why don't know what the labels are by simply trying to find ways to Closter things together nearby and then use the Clusters to assign labels to new data and we're learning models by looking at label data and seeing how do we best come up with a way of separating with a line or plane or a collection of lines examples from one group from examples of the other group with the acknowledgement that we want to avoid overfitting, we don't want to create a really complicated system and, as a consequence, we're going to have to make some trade offs between what we call false, positive and false negative for the resulting classifier recommend label any new data by just deciding where you are with respect to that separating line. So here's what you're going to see the next two and a half lectures every machine.  Learning method has five essential components.  We need to decide what's the training date on.  How are we going to evaluate the success of that system already seen some examples of that?  We need to decide.  How are we going to represent each instance that we're giving?  I happen to choose height and weight for football players, but I might have been better off to pick average speed, or I don't know, arm like something else.  How do I figure out what are the right features and associated with that? How do I measure distances between those features?  How do I decide what's closed? And what's not close?  Maybe you should be different in terms of weight versus height, for example.  I need to make that decision and those are the two things are going to show you examples up to date how to go through that starting next week.  Professor, good Tigers going to show you how you take those and actually start building more detailed versions of measuring clustering measuring similarities to find an objective function that you want to minimize to decide.  What is the best cluster to use? And then what is the best optimization methods? You want to use to learn that model? So let's start talking about features.  I got a set of examples labeled or not.  I need to decide.  What is it about those examples that useful to use When I want to decide what's close to another thing or not, one of the problems is?  If it was really easy, it would be really easy features.  Don't always capture what you want.  I'm going to belabor that football analogy, But why did I pick height and weight, cuz?  It was easy to find, But what is you know if you work for the New England Patriots?  What is the thing that you really look for?  When you're asking, what's the right feature is probably some other combination thing, so you, as a designer have to say, what are the features?  I want to use a quote by the ways from one of the great statisticians of the 20th century, which I think captures it will so feature engineering as you as a programmer comes down to deciding both. What are the features?  I want to measure in that back door thing going to put together. And how do I decide relative ways to wait? So John and Anna and I could have made our Arch or this country or wrong turns on our job.  This term, really easy If we had sat down at the beginning of the term, has said. You know, we've talked This course. Many times we got data from.  I don't know Jon.  Thousands of students probably over this time. Let's just build a little learning algorithm. It takes a set of data and predicts your final grade.  You don't have to come to class.  Don't have to go through all the problems. That's we'll just predict your final grade will not be nice being make our job a little easier and you may or may not like that idea, but I could think about predicting that crate. And why am I telling you this example?  I was trying to see if I could get a few.  Smiles, I saw a couple of them there, but think about the features. What would I measure?  I shall put this on, John.  Cuz it's his idea.  What would he measure well?  Gpa's probably not a bad predictor performance.  You do well in other classes.  You're likely to do well in this class.  I'm going to use this one very carefully.  Prior programming experience is at least a predictor, but it is not a perfect predictor.  Those of you having program before in this class, you can still do really well in this class. But it's an indication that you've seen other programming languages on the other hand.  I don't believe in astrology, so I don't think the month in which you're born. The aastra astrological sign under which you were born has probably anything to do with.  How will you program?  I doubt that I color has anything to do with.  How will you program you get the ideas? Some features matter others, don't no!  I could just throw all the features in and hope that the machine learning algorithm sorts out those it wants to keep from those it doesn't, but I remind you that idea of overfitting if I do that. There is the danger that it will find some correlation between birth month. I color and GPA, and that's going to lead to a conclusion that we really don't like by the way. In case you're worried.  I can assure you that.  Stu Schmill, in the dean of Admissions Department, does not use machine learning to pick you.  He actually looks at a whole bunch of things because it's not easy to replace him with the machine.  Yeah, by so what this says is we need to think about.  How do we pick the features and mostly What we're trying to do is to maximize something, called the signal to noise ratio.  Maximize those features that carry the most information and remove the ones that don't so.  I want to show you an example of how you might think about this.  I want a label reptiles.  I want to come up with a way of labeling animals, as are there a reptile or not, and I'll give you a single example with a single example.  You can't really do much, but he, from this example.  I know that a cobra is it lays eggs. It has scales as poisonous as cold.  Blooded has no legs and it's a reptile. So I could say my model of a reptile is well.  I'm not certain I don't have enough data yet, but if I give you a second example, and it also happens to be egg laying, have scales, poisonous, cold blooded, no legs. There's my model right, perfectly reasonable model, Whether I designed it or machine learning Autumn would do it says if all of these are true label, it is a reptile. Okay, and now I give you a boa.  Constrictor, it's a reptile, but it doesn't fit the model in a particular, it's not eggling, and it's not poisonous, so I got to refine the model or the algorithms got to refine the model in this.  I want to remind you looking at the feature, so I started out with five features.  This doesn't fit so probably what I should do is reduce. It will look at Scales. I'm going to look at Cold.  Blooded I'm going to look at likes That captures all three examples and can even think about this in terms of clustering.  All three of them would fit with that okay now.  I give you another example chicken.  I don't think it's a reptile.  I'm pretty sure it's not a reptile, and it nicely still fits this model right because while it has scales, but you me and not realized it's not cold blooded, and it has like so it is a negative exam.",
    